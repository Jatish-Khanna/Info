9.
 Metadata Partitioning
To scale out metadata DB, we need to partition it so that it can store information about millions of users and billions of files/chunks.
 We need to come up with a partitioning scheme that would divide and store our data to different DB servers.


1.
 Vertical Partitioning: We can partition our database in such a way that we store tables related to one particular feature on one server.
 For example, we can store all the user related tables in one database and all files/chunks related tables in another database.
 Although this approach is straightforward to implement it has some issues:

Will we still have scale issues? What if we have trillions of chunks to be stored and our database cannot support to store such huge number of records? How would we further partition such tables?
Joining two tables in two separate databases can cause performance and consistency issues.
 How frequently do we have to join user and file tables?
2.
 Range Based Partitioning: What if we store files/chunks in separate partitions based on the first letter of the File Path.
 So, we save all the files starting with letter ‘A’ in one partition and those that start with letter ‘B’ into another partition and so on.
 This approach is called range based partitioning.
 We can even combine certain less frequently occurring letters into one database partition.
 We should come up with this partitioning scheme statically so that we can always store/find a file in a predictable manner.


The main problem with this approach is that it can lead to unbalanced servers.
 For example, if we decide to put all files starting with letter ‘E’ into a DB partition, and later we realize that we have too many files that start with letter ‘E’, to such an extent that we cannot fit them into one DB partition.


3.
 Hash-Based Partitioning: In this scheme we take a hash of the object we are storing and based on this hash we figure out the DB partition to which this object should go.
 In our case, we can take the hash of the ‘FileID’ of the File object we are storing to determine the partition the file will be stored.
 Our hashing function will randomly distribute objects into different partitions, e.
g.
, our hashing function can always map any ID to a number between [1…256], and this number would be the partition we will store our object.


This approach can still lead to overloaded partitions, which can be solved by using Consistent Hashing.

Twitter is one of the largest social networking service where users can 
  share photos, news, and text-based messages. In this chapter, we will design a service that can store and search user tweets.

1) What is twitter search ?
    . User can update there status or post a tweet
    . The post contains only text
    
    Goal is to design a system that search over all the statues of users
    
2) Requirements?
  Functional requirements?
    Lets assume twitter has 1.5 billion users and 800 Million users
    . Average 400 Million tweets every day posted by users
    . Average size of tweet is 300 byte i.e. 150 characters considering each character represent 2 byte
    . Number of searches per day 500 Million searches per day
    . Define each search query (keywords and operators)
    
Non-Functional requirements?
  . Availability
  . performance
  . pagenated
  
3) Estimations?
  Storage estimates?
    . since 400Million tweets has been posted everyday
    400 Million tweets per day * 300 byte (tweet size) = 120 GB/day
    
    120 GB / 86400 = 1.38 Mb/sec
    
  Network bandwidth?
    . New post - 1.38 Mb/s 
    
4) System API?
  search(apiDevKey, searchTerms, maximumResultToReturn, sorted, pageToken)
  
  Parameters:
api_dev_key (string): The API developer key of a registered account.
 This will be used to, among other things, throttle users based on their allocated quota.

search_terms (string): A string containing the search terms.

maximum_results_to_return (number): Number of status messages to return.

sort (number): Optional sort mode: Latest first (0 - default), Best matched (1), Most liked (2).

page_token (string): This token will specify a page in the result set that should be returned.


Returns: (JSON)
A JSON containing information about a list of status messages matching the search query.
 Each result entry can have the user ID & name, status text, status ID, creation time, number of likes, etc.


5) High Level design?
  . We need to store all the statuses in the database, 
  and indexing to be done to define which word appears in which status
  
  . Indexing with the words in the tweet will help us in quickly returning all the statuses stored in the database
  
  
6) Detailed level design?
  A) Storage?
    As calculated above, to store 120Gb per day. this data storage should be efficiently distrubuted among multiple servers
    
    120GB * 5 years * 365 days in a year = 200 TB
    
    If we never want to be more than 80% storage - 
    250 TB per five years
    
    
    Note: The above storage is just to store tweets posted by user
    Extra storage is required -
      . To build indexes of keywords in tweets
      . To Store user related information
      . Caching the tweets
      . Replication of the server (to have fault tolerant system)
      
  Assuming modern server can store upto 4Tb of data, we need 125+ such servers to store complete information
  
  How can we create system wide status ID's?
    If we have 400 Million tweets per day, then how many would be there in  5 years
    
    400 Million * 5 years * 365 days = 730 billion
    
    
    This means we need at least five byte number to store ID information for five years.
    An additional service that can help us in generating the unique status ID, whenever we need to store object
    
    We feed the status ID to the servie which will be hashed to find our storage server location
    

  B) Index?
    How should are indexes look like to efficiently return the results to query posted by user?
      . These indexes defines which word comes in which object
      . If we want to build an index for all the words in english- 
        300K words in english dictionary?
          and 200 K nouns, a total of 500 K words to be indexed by the database
          
       . Assuming average word size in english or indexes would be 5 characters
       
       500 K * 5 = 2.5 Mb
      These indexes can be easily store in memory
      
   C) Status ID?
    We have 400 Million tweets per day
    400 million * 2years * 365 days * 5 byte to store status ID = 1460 Gb
    
    
    How the indexes looks like?
      A hashtable with Keys as word indexes and values as list of status ID's
      
      . Assuming we have 15 words to be indexed per tweet
      (1460 space required to store status ID for two years * 15 each tweets object been refered by keys) + 2.5 Mb = 21 TB
      
    Assuming a high end server has 144 Gb of memory and 21 TB can be shareded among 152 such servers
    
    
 7) Sharding of database?
  A) Sharding based on word?
    We build index based on words i.e. 500 K words from english dictionary including nouns
    
    . We iterate through each word of tweet and hash that word to find the server where index has been stored
    . Append the Index ID to list
    . While searching each word, we query particular server by hashing the search query
    
    Issues?
      . What if a word becomes hot? then we have unbalanced shard where multiple request to be served by single server
      . Maintaining a non-uniform distribution of servers is also an issue
      
      Solution?
        Use consistent hashing
        
  B) Sharding based on Indexed object?
    Sharding of data will be done based on status ID
    . Pass status Id to our hash function to find the server
    . index all the words of tweet on the server
    
    Query or search?
      . Query each server for the word
      . Each server will return the set of status Id's
      . An aggregator service can combine results from the user
      
      
 8) Fault Tolerant?
  What will happen when the indexed server dies?
    . We can have a secondary replica of each server which is synchronizing with the primary
    . If primary server has been crashed, elect the secondary server as primary 
    
    . What if both the servers goes down?
      . We need to rebuild index on the new server
      . Iterate over each Status ID from the database and apply hash function
      . If the status ID hashes to particular server we use it
      
      This is inefficient way to restore the complete index in case of outage
      
      
      Instead of building using brute force techinque, use reverse index strategy?
        . A reverse index that will map all the status ID to index server.
        This information can stored on index builder server
        
        A Hashtable represents Key as index server number and values as HashSet of status ID's
        In case of a rebuilt indexing is required, it can simply rebuilt new server   using this metadata
        
        
        
9)  Cache?
   80-20% rule with the LRU eviction policy, where 80% of traffic generated by 20% query
   These queries can be cached to imporve the performance of system
   
10) load balancing?
    We can add load balancing layer in ouor system
    a) between client and application servers
    b) Between application servers and backend servers
    
    A round robin technique to distribute the traffic among the backend servers
    
 11)  Ranking
How about if we want to rank the search results by social graph distance, popularity, relevance, etc?

Let’s assume we want to rank statuses on popularity, like, how many likes or comments a status is getting, etc.
 In such a case our ranking algorithm can calculate a ‘popularity number’ (based on the number of likes etc.
), and store it with the index.
 Each partition can sort the results based on this popularity number before returning results to the aggregator server.
 The aggregator server combines all these results, sort them based on the popularity number and sends the top results to the user.

    














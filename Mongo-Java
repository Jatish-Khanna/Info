Build MongoDB Java driver -
  . Peformant 
  . resilient
  . Robust 

mongorestore
A utility for importing binary data into MongoDB.
mongo
The shell for exploring data in MongoDB.

URL to connect to MongoDB -
 mongodb://<USER>:<PASSWORD>@<HOST>/<database>?retryWrites=true
  
  HOST - is the host name of SRV server.
   The HOST is not a database server, it only holds the SRV(Service) records.
   It has a list of DNS which only SRV can resolve
   
   This is used when Servers in Clusture change, which gives privilege not to update anything on client when clusture changes
 
 
 retryWrites - To try writes if error while writing the documents to DB
 
 The --username and --password flags cannot be used with and SRV string. 
 Instead, include the username and password in the SRV string 
 (i.e. mongodb+srv://USER:PASSWORD@<your-cluster-address>)
 
 ---------------------  
   
   MongoDB drivers Java-
   
   1) <groupId>org.mongodb</groupId>
      <artifactId>mongo-db-sync</artifactId>
      
   2) 
    <groupId>org.mongodb</groupId>
      <artifactId>mongo-db-async</artifactId>
      
---------------------     
 Driver Base Classes -
   1) MongoClient - connection b/w app and Clusture
   2) MongoDatabase
   3) MongoCollection
   4) Document
   5) Bson
   
---------------------  
Connection in action, Can you code it?
  // MongoClients Builder provides a conncetion to the provided in the uri string

  MongoClient mongoClient = MongoClients.create(uri)
   
 // The client configuration can be done using MongoClientSettings
 // The instance can be extracted from MongoClient instance
 // This class offers a builder method - that allows to create multiple clients with different options
 MongoClientSettings mongoClientSettings;
 
 ConnectionString connectionString = new ConnectionString(uri);
 mongoClientSettings = MongoClientSetting.builder()
                                          .applyConnectionString(connectionString)
                                          .applyName("TestName")
                                          .applyConnectionPoolSettings(
                                            builder -> builder.maxTimeWait(1000, TimeUnit.MILLISECONDS)
                                          .build();
 MongoClient mongoClient = MongoClients.create(mongoClientSettings);
   
   
---------------------  

List all the Database names using the created MongoClient connection
  MongoIterable<String> databaseListIterable = mongoClient.listDatabaseNames();
   
Note: MongoIterable<T> is exhaustible like a Cursor or ReadAHead queue. 
  - So store the data persisted by MongoIterable in Custom DS/Collections
   
---------------------  

Get Mongo Database from the MongoClient

  MongoDatabase mongoDatabase = mongoClient.getDatabase("DB name");
   
---------------------     
 Get Mongo Collection in the database
 
   MongoCollection mongoCollection = mongoDatabase.getCollection("Collection_NAME");
   
 ---------------------  
 
 Find the data in MongoCollection i.e. execute DML
 
    MongoIterable<Document> mongoFullDB = mongoCollection.find().limit(10);
   
---------------------  
A simple Document in Java

{
  "Top" : {
    "First Key" : "value"
  },
  "Key2" : "value"
}
  Document document = new Document(
                    "Top", new Document("First Key", "Value").append("Key2", "Value"));

  mongoCollection.insertOne(document);

NOte: Documents are Bson datastructures thats why Document implements Bson interface in the API

----------------------

Find Using Java classes -

 Document findOverFilter = new Document("FILTER_KEY", "FILTER_VALUE");
 Document document = mongoCollection.find(findOverFilter);

or 

Bson findUsingBson = eq("FILTER_KEY", "FILTER_VALUE")

 Document document = mongoCollection.find(findUsingBson);

-----------------------

Find in an Array Using Java

 Document findOverFilter = new Document("FILTER_KEY", new Document(
                                                            "$all", Arrays.asList("Key1", "Key2")));
 List<Document> resultList = new ArrayList<>();
 mongoCollection.find(findOverFilter)
                                    .into(resultList);

or 

Bson findUsingBson = all("FILTER_KEY", "FILTER_VALUE1", "FILTER_VALUE2")
List<Document> resultList = new ArrayList<>();
mongoCollection.find(findUsingBson).into(resultList);
 
-----------------------------

A complex query using Java with multiple operators

Bson findUsingBson = and(
                      eq("FILTER_KEY", "FILTER_VALUE"),
                      eq("FILTER_KEY1", "FILTER_VALUE1"),
                      gte("FILTER_KEY2", "FILTER_VALUE2"),
                      lt("FILTER_KEY3", "FILTER_VALUE3"));
                      
List<Document> resultList = new ArrayList<>();
mongoCollection.find(findUsingBson).into(resultList);
 
-----------------------------

Projections using Java 

    Document oldFilter = new Document("FILTER_KEY", "FILTER_VALUE");
    Document oldResult =
        moviesCollection
            .find(oldFilter)
            .limit(1)
            .projection(new Document("projection_key", 1).append("projection_key_2", 1))
            .iterator()
            .tryNext();
 
 or
 
 
     Bson queryFilter = and(eq("FILTER_KEY", "FILTER_VALUE"));
    Document result =
        moviesCollection
            .find(queryFilter)
            .limit(1)
            // this feels much more declarative
            .projection(fields(include("projection_key", "projection_key2"))) // , exclude("_id") to exclude field or excludeId()
            .iterator()
            .tryNext();
 
 -------------------------
 
 Why Using POJO and Custom Codecs -
 
   1. Allows us to write a cleaner code
   2. In our example the _id is a string and not an ObjectId in Java as in Mongo DB, 
     that is a mismatch of data types
     
  With the CustomCodec we can convert a document to Object and vis-a-viz
  
  MyCustomCode customCodec = new MyCustomCodex();
  
  // Register Codec and create a registry instance
 CodecRegistry codecRegistry = fromRegistries(
                                          MongoClientSettings.getDefaultCodecRegistry(), fromCodecs(customCodec));
                                          
// Get a Bson Document
//Bson queryFilter = new Document("FILTER_KEY", "FILTER_VALUE");
Bson queryFilter = Filters.all("FILTER_KEY", "FILTER_VALUE");
 
// To access Collection with Custom Codec and mapping to BeanClass
MongoColelction<BeanClass> mongoCollection = mongoDatabase.getCollection("COLLECTION_NAME", BeanClass.class)
                                                          .withCodecRegistry(codecRegistry);
 
 
 // Retrieve Documents - 
 BeanClass foundDocument = mongoCollection.find(Filters.eq(queryFilter)).first();
 
 //Note: What else we get? we get an Update on Codec
 
  foundDocument.setSomeInstance("SOME VALUE");
  
  // Insert the updated document;
  mongoCollection.insertOne(foundDocument);
 
 -------------------------
 
 allows us to use the default CodecRegistry while customizing the
 fields that we know need special treatment. For example, if you have
 a field that would need type conversion, or has various different
 data types in it. This needs to be addressed when the documents are
 read into your Java application, here is another approach to handling
 it.
  
 // select a class that will be used as our POJO
    ClassModelBuilder<CustomBean> classModelBuilder =
        ClassModel.builder(CustomBean.class);
        
    // get the property that needs type conversion
    PropertyModelBuilder<String> idPropertyModelBuilder =
        (PropertyModelBuilder<String>) classModelBuilder.getProperty("id");
        
    // apply type conversion to the property of interest
    // StringObjectIdCodec describes specifically how to encode and decode
    // the ObjectId into a String and vice-versa.
    idPropertyModelBuilder.codec(new StringObjectIdCodec());
    
    
    // use the default CodecRegistry, with the changes implemented above
    // through registering the classModelBuilder with the PojoCodecProvider
    CodecRegistry codecRegistry =
        fromRegistries(
            MongoClientSettings.getDefaultCodecRegistry(),
            fromProviders(
                PojoCodecProvider.builder()
                    .register(classModelBuilder.build())
                    .automatic(true)
                    .build()));
                    
    // Get the data from the COllection
    MongoCollection<CustomBean> mongoCollection =
        mongoDatabase
            .getCollection("COLLECTION_NAME", CustomBean.class)
            .withCodecRegistry(codecRegistry);
            
    // build a search query
    Bson queryFilter = all("FILTER_KEY", "FILTER_VALUE");
    // find the actor that we want to test for
    CustomBean foundCollection = actors.find(Filters.eq(queryFilter)).iterator().tryNext();
    
 
 ----------------------------
 Benefits of Using CustomCodec POJO and/ Field Customization approach
 
 Pojo Implementation is an elegante way to define Codecs to convert b/w Documents and Collection Class
 @BeanProperty - to map Collection field to the Class field
 Automatic Interospection i.e. Automatic analyzing of Beans design and map to Collection
 Generics and Sub-documents are easier to maintain with POJO, instead of writing separate methods to traverse sub-documents
 
 
-----------------------------

Using Simple Aggregations -

Bson someFilter = Filters.eq("KEY", "Value");
Bson matchedDocuments = Aggregates.match(someFilter);

List<Bson> pipeline = new ArrayList<>();
// add the matchStage to the pipeline
pipeline.add(matchedDocuments);

// execute the aggregate() collection command.
AggregateIterable<Document> getResultDocuments = <COLLECTION>.aggregate(pipeline);

// Collect all the documents
List<Document> finalList = new ArrayList<>();
getResultDocuments.into(finalList);

----------------------------------

Aggregation Multiple Stages -


Bson someFilter = Filters.eq("KEY", "Value");
Bson matchedDocuments = Aggregates.match(someFilter);

// Just another filter to flat the array
Bson unwindDocuments = Aggregates.unwind("$someArray");


/*Group operations are in place to do some sort of accumulation
    operation.
    Operations like $sum, $avg, $min, $max ... are good candidates to be
    used along side group operations, and there is a java builder for that.
    @see com.mongodb.client.model.Accumulators handles all accumulation 
*/
// use $sum accumulator to sum unwind documents
BsonField sumAccumulator = Accumulators.sum("count", 1);

// adding both group _id and accumulators
Bson groupStage = Aggregates.group("$someArray", sumAccumulator);


    // create the sort order using Sorts builder
    Bson sortOrder = Sorts.descending("count");
    // pass the sort order to the sort stage builder
    Bson sortStage = Aggregates.sort(sortOrder);

    pipeline.add(matchedDocuments);
    pipeline.add(unwindDocuments);
    pipeline.add(groupStage);
    pipeline.add(sortStage);


// execute the aggregate() collection command.
AggregateIterable<Document> getResultDocuments = <COLLECTION>.aggregate(pipeline);

// Collect all the documents
List<Document> finalList = new ArrayList<>();
getResultDocuments.into(finalList);

-------------------------------------

find().limit() -

Use the limit() method on a cursor to specify the maximum number of documents the cursor will return. 
limit() is analogous to the LIMIT statement in a SQL database.
  :Zero Value
  A limit() value of 0 (i.e. .limit(0)) is equivalent to setting no limit.
  :Negative Values
  A negative limit is similar to a positive limit but closes the cursor after returning a single batch of results. 
As such, with a negative limit, if the limited result set does not fit into a single batch, 
the number of documents received will be less than the specified limit.
By passing a negative limit, the client indicates to the server that it will not ask for a subsequent batch via getMore.


cursor.batchSize(size)
  Specifies the number of documents to return in each batch of the response from the MongoDB instance.
  Specifying 1 or a negative number is analogous to using the limit() method.

Note: Java driver default batchSize is 0
  That means it will use the batchSize defined by server i.e. 101

---------------------------------------

Sorting in MongoDB
  Bson sortDescendingStage = Sorts.descending("KEY_TO_SORT_ON")
  
  Iterable<Document> sortedCollection = <COLLECTION_NAME>.find().sort(sortDescendingStage);
  
  
Skip Documents -

cursor.skip() method on a cursor to control where MongoDB begins returning results. 
This approach may be useful in implementing paginated results.
  
    Iterable<Document> sortedCollection = <COLLECTION_NAME>.find().skip(10);
  
NOte: not using the sort order, actually sort the document as insertion order not the values of fields  
  
-------------------

The sort, skip and limit method are also avaialble as a part of aggregation pipeline
Why we need for aggregation pipeline?
  These methods are required to generate intermediatory or results from the pipeline
  
Aggregates.match(FILTER)
  Aggregates.sort(KEY)
  Aggregates.skip(NUMBER)
  Aggregates.limit(NUMBER)
  
  and use these using pipeline
  
  1. Add to the pipeline 
  2. execute the pipeline using <COLLECTION_INSTANCE>.aggregate(pipeline)
  
------------------------

Basic writes -
  Document document = new Document("KEY", "VALUE");
  
  // Sets a particular order in the document
  document.append("KEY2", "VALUE2");
  // Replaces values if it exists, keeping the original order of fields
  document.put("KEY3", "VALUE3");
  
  Note:
    How insertOne ensures that document has been inserted into DB?
      On failure it throws MongoWriteException
  
---------------------------------------  
  
  Dcoment document = new Document();
  document.append("OLD_KEY", "NEW_VALUE");
  
  // replace the document with the found _id
  <COLLECTION_INSTANCE>.replaceOne(eq("_id", "OBJECT_ID"), document);
  
  // In the above the complete document is replaced i.e. 
  // Instead of updating the data for a particular field, other fields have been removed 
  
  updateOne:
  
  // update the document with the found _id, only specific keys not the complete document
  // Using the set() operator
  <COLLECTION_INSTANCE>.updateOne(eq("_id", "OBJECT_ID"), set(document));
  
  UpdateMany:
  
  // update multiple documents with the filter, only specific keys not the complete document
  // Using the set() operator
  <COLLECTION_INSTANCE>.updateMany(eq("KEY", "VALUE"), set("KEY_UPDATE_INSERT", "VAL"));
  
  
  updateOne: works the same way
  UpdateMany:
  
  // update multiple documents with the filter, only specific matching filter
  // Using the unset() operator
  <COLLECTION_INSTANCE>.updateMany(eq("KEY", "VALUE"), unset("KEY_TO_UNSET"));
  
  ------------------------------------
  
  MongoDB Joins from different Collections-
    from: The target Collection
    let: the local joinfield
    pipeline:
    as: The name for the result
  
   "$lookup" :{
     from: "COLLECTIONS",
     let: { "id" : "$_id" },
     pipeline: [
       {
         "$match" : {
            "$expr" : {
              "$eq" : [
                "$local_id", "$$id"
              ]
            }
         }
       }
     ],
     as: "Join_NAME"
   }
  
  Description of Join above mentioned:
  
  // The Collection which is Joining doesn't have access to the fields of Other Collection to be Joined with
  // Unless these have been specified in "let:"
  // like we declared variables "id" and assigned it $_id from Other Collection
  // The variable can be Used in "pipeline:" section
  // To use the declared variable from other collection use it with "$$" eg. $$id
  // Fields from this collection will be used using "$"
  
  // In the Pipeline
  // Match operator and under that expr operator is used
  // Eq operator to check local_id (local field) with the foriegn field($$id defined in let)
  
   "$lookup" : {
     from: "COLLECTIONS",
     let: { "id" : "$_id" },
     pipeline: [
       {
         "$match" : {
            "$expr" : {
              "$eq" : [
                "$local_id", "$$id"
              ]
            }
         }
       },
       {
         "$count" : "count"
       }
     ],
     as: "Join_NAME"
   }  
  
  //Description of Join above mentioned:
  // In addition to last Join,
  // Pipeline is extended with Just to count the number of Documents Joined for each Document from Other Collection
  
 ---------------------------------------- 
  
  Delete document in MongoDb - 
  
  // delete  first natural Order Found record [InsertionOrder is the Natural order of the documents]
  // Store the response from MongoDB in DeleteResult reference
  DeleteResult deleteResult = <COLLECTION_INSTANCE>.deleteOne();
  
  System.out.println(deleteResult.getDeletedCount());
  
  
  --------------------------
  // Get the deleted document back
  Document deletedDocument = <COLLECTION_INSTANCE>.findOneAndDelete("FILTER");
  
  













Custom application configration for containerized images:
  ConfigMap:
    A k8s Object to seprate configuration from Pods
    It stores the configuration data in Key/value pairs within ETCD database
    Configuration Files
    Command Line Arguments
    Environment Variables
    
    But the sensitive data, will be managed using Secrets.
    ConfigMap should be created before referencing in the Pod spec 
    
kubectl create configmap <map-name> <data-source>

<data-source>|-
  -from-file directory or file name 
  --from-literal Key/value pair
  
kubectl get configmaps <map-name> -o wide|yaml
kubectl describe configmap <map-name>
  
Example - manifest:

apiVersion: v1
kind: Pod
metadata:
  name: redis
spec:
  containers:
    - name: redis-container
      image: redis/redis:latest
      volumeMounts:
        - name: config
          mountPath: /redis-master
  volumes:
    - name: config
      configMap:
        name: my-config-map
        items:
         - key: redis-config  # file name from the config-map
           path: redis.conf

kubectl create -f redis-pod.yaml
kubectl exec redis cat /redis-master/redis.conf

========================================================

ConfigMap for Literal Values:
  kubectl create configmap literal-map --from-literal=my.key=value
  kubectl get configmap literal-map
  
  Reference the literal Key/values
  
  apiVersion: v1
  kind: pod
  metadata:
    name: map-pod
  spec:
    containers:
      - name: busy-box-test
        image: busybox
        command: ["/bin/sh", "-c", "env"]
        env:
          - name: CUSTOM_KEY
            valueFrom:
              configMapKeyRef:
                name: literal-map
                key: my.key
    restartPolicy: never  # never restart the pod
    
  kubectl create -f my-busy-pod.yaml
  kubectl logs map-pod
  
  ==========================================
  
  K8s - DeamonSet
    To control that all or some nodes of the cluster runs a copy of the Pod
    Example: An application instance per instance on all the nodes in the cluster or subset of nodes, even when the cluster is scaling
    Subsets - can be applied with the help of node labels. 
    Typical UseCases?
      Storage Deamons: ceph
      Log collection Daemon: fluentd
      Node monitoring deamon: collectd
      
Example manifest file:

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: custom-ds
  labels:
    tier: daemons
spec:
  template:
    metadata:
      labels:
        app: fluentd   # to be used in the selector section
    spec:
      containers:
        image:
          - name: fluent-container
            image: gcr.io/google-containers/fluentd-elasticsearch
  selector:
    matchLabels:
      app: fluentd
  
kubectl create -f custom-daemon-set.yaml
kubectl get po -o wide
kubectl get ds
kubectl describe ds custom-ds
ubectl delete ds custom-ds   # make sure to delete the daemonset not pods
===============================================

Manage Secrets in the K8s: A sensitive store
    Prevent accidential disclosure of accidential data
    The secrets doesn't depend on the Pod but used by Pod
    ETCD a Key/Value data store has been used to persist the secrets
  Limits: 1MB max. per secret
  Secrets are only sent to nodes in a cluster where it is required.
  Secret is stored in /tmp-fs volume, 
  
  Inject Secrets inPods:
  1) Volumes
  2) Environment variables
  
1) Using Kubectl command line tool to create Secrets [preferred]
kubectl create secret [TYPE] [name] [data]
   Type:
    a) Genecric:
      . File  - --from-file
      . Directory - --from-file
      . Literal Value - --from-literal
      
    b) docker-registry
    c) tls
  
Example:
  kubectl create secret generic app-secret --from-file=./my-app-secrets.conf [--from-file=...]
  kubectl get secrets
  kubectl describe secrets app-secret   # Actual values are not displayed in the output

---------------------------
2) Create Secrets Manually -
  step1: convert to base64 format
  spte2:
  
  apiVersion: v1
  kind: Secret
  metadata:
    name: app-secret-mf
  type: Opaque     #   
  data:            # map of key value pairs
    u-password: 1fdnjkaf       
    a-password: ear12jEf

kubectl create -f my-secret.yaml

----------------------------------------------

Decode the Secrets:
  kubeclt get secrets app-secret-mf -o yaml
  echo "value" | base64 --decode

------------------------------------

Consuming Secrets?
  1) Volumes:
    
apiVersion: v1
kind: pod
metadata:
  name: c-pod
spec:
  containers:
    - name: c-pod
      image: redis
      volumeMounts:
        - name: my-secret-mount
          mountPath: /app/secret.config
          readOnly: true
   volumes:
     - name: my-secret-mount
       secret:
         secretName: app-secret-mf
         
kubectl create -f my-app.yaml
kubectl get po -o wide

-------------------------------------------

2) Consuming Secrets using Volumes:
  
apiVersion: v1
kind: Pod
metadata:
  name: app-secrets-env
spec:
  containers:
    - name: r-container
      image: redis
      env:
        - name: SECRET_PASS
          valueFrom:
            secretKeyRef:
              name: app-secret-mf
              key: a-password
        - name: SECRET_U_PASS
          valueFrom:
            secretKeyRef:
              name: app-secret-mf
              key: u-password
  restartPolicy: never

kubectl create -f my-app.yaml
kubectl get po
=============================================

Kubernetes Scheduling Jobs?
  A controller to setup the Pods for scheduling and completion
  
  1) Run to completion (batch-processing) -
    It waits for task to be completed and waits for exit=0 from the Pod.
    Once received the signal Pod will be moved from Running to Shutdown state
    K8s won't delete these Pods, and these has to be manually deleted, to review the logs, warning...
    Pods in shutdown won't consume any resources
    On intermittent failures, a new pod will be started but application has to handle the restart appropriatly
      i.e. The K8s master will take care of Pod to be running
   
  2) Scheduled (CronJob) -
    To schedule a job for specific time as same as Linux schedule
    
    
Example Manifest for Run to completion Job:
apiVersion: batch/v1
kind: Job
metadata:
  name: app-rtc-job
spec:
  template:
    metadata:
      name: pod-rtc-job 
    spec:
      containers:
        - name: app-container
          image: centos:7
          command:
            - "bin/sh"
            - "-c"
            - "for element in 1 123 34 5 2 21 1 32 1 13 21; do echo $element; done;"
       restartPolicy: Never   

kubectl create -f sample-pod.yaml
kubectl get jobs -o yaml|wide
kubectl get po
kubectl logs app-rtc-job-scda
kubectl describe jobs app-rtc-job
kubectl delete job app-rtc-job
====================================================

1) Web crawler?
  The software that systematically downloads the world wide web.
  Web crawlers are known as spiders, bots, worms, walkers, robots
  
  It collects document by fetching links from the source or start page
  Use cases: search engines use web crawlers to download and index web pages to perform search queries faster
  
  . To test web pages and links for valid syntax or structure
  . To monitor sites if there are any changes in structure
  . Maintain mirror sites for popular web pages
  . Copyright infringments
  . To build a special purpose index
  
2) Requirements?
  Functional requirements?
    Crawl and index the web pages from source or world wide web
    
  Non-Functional requirements?
    Scalability: It should be able to fetch millions of webpages
    Extensibility: to crawl and index new webpages
        To index darknet or darkweb
        
3) Design considerations?
  What type of webpages to be crawled over WWW? The design depends on the media type of crawling.
    If it is expected to explore only HTML content or multi-media type content
    The parsing of content depends on the multimedia type
    
  What protocols have been supported by crawler?
    . HTTP, FTP or ...
    
  What will be the upper bound on the database - The crawler has an upper bound of 15 billion web pages
  
  What is "RobotExclusion"? Webcrawler implement Robot exclusion protocol restricts webpages or its parts from crawling
  It  depends on a special document called robot.txt containing these declarations before downloading any real content
  
4) Capacity estimations?
  A) An upper bound of 15 billion web pages to be indexed in 1 Months 
  15 Billion / (4 weeks * 7 days * 86400) =  6200 pages per second
  
  B) Storage estimations?
      . Assuming an average page holds the content of 100KB and 500 bytes of metadata information
        15 billion * (100 KB + 500 byte) = 1.5 PB
        
        . 70% capacity model i.e. at given point the storage should have 30% available space
            1.5 + (.3 * 1.5) =  2.14 PB


5) High level design?
  The basic functionality of crawler it to work on source or seed URL and perform all the mentioned steps
  
  . Pick a URL from the unvisited list
  . Establish a connection with the domain
  . Download the corresponding document
  . Parse the document and look for source URI's i.e. connections to other webpages within domain or outside
  . Add all the unvisited URL's to the list
  . Process the document and create index on words
  
 Crawling Algorithms?
  A) Breadth first search?
    A hybrid method to perform the crawling operation is used.
    Where BFS is used to crawl all the external links
    DFS is used to crawl all the webpages within the same website
    
    
  B) Path-ascending crawling?
    This allows to discover lot of isolated resources or resource with no inbound links found in regural practise
    In this crawler will ascend to every path  in each URL
    
  
  Issues implementing a regular crawler?
    Two most important things to take care of while implementing a web crawler
    A) Large volume of webpages?
      A large volume of webpages implies that a webcrawler can download a fraction of webpage 
      . Web crawler should know how to priortize the downloads
      
    B) Rate of change on web pages?
      Another problem with the webpages is they are dynamic, i.e. changes are frequent
      
   Basic features a web crawler should have?
    A) URL Frontier?
      The crawler should know priortizing the downloading of webpages
    B) HTTP Fetccher?
      To retrieve webpage
    C) Extractor? Parse HTML webpage to extract indexes, content and outbound URL's
    D) Duplicate Eliminator? Should take care of duplicate content
    E) DataStore? To store data retrieved and build index to be served to its clients
    
   
   
6) Low level design?
  A crawler is running on multiple server and the steps performed to build indexes are-
    . Get the URL from the frontier for which crawling will be initiated
    . The Fetcher or particularly HTTP fetcher has been called to download the webpage
    . Downloaded document is placed on the stream to parse and extract the content
    . The Extractor check if the document has already been parsed running de-dup test
    . New URL has been extracted from the content
    . A de-dup test to find if similar URL has been download or in the processing queue
    . In case of valid URL, a URL has been appended to URL frontier with the assigned priority
    
    Components?
     A) URL Frontier?
      This contains all the URL's that are queued to be downloaded by the HTML or other category of fetcher
      . We use DFS or BFS technique to visit the webpage
      Multiple URL frontier can be started to cater huge list of URL's to visit
      
      Requirements to implement web crawler?
        A crawler should rate limit the number of request to server
        Multiple API crawler should not connect to same target web server
        
      Solution?
        A crawler can have collection of distinct FIFO sub-queues  on each server
        Each worker thread has it's sub-queue
        When a URL need to be added to one of the sub-queue is determined by canonical hostname
        A hashmap can map each hostname to thread number and it's corresponding sub-queue
        
        Note:
          Using FIFO queue will not issue multiple request to webserver
          and a sub-queue owned by single thread will issue request to target server
          
       What is the size of frontier?
        We have thousands of URL to be crawled, so a secondary storage is important suggests the list to parse 
        . Enqueue buffer? It calculates URL's in batches and store in buffer
        Once the buffer is full, the content is flushed to disk
        
        . Dequeue buffer? It cache or buffers a list of URL to be traversed, it reads a batch of URL's from secondary storage
        
        
        
   B) Fetcher Module?
    This module helps download the content from the webpage, an extention of fetcher is its protocol specific implementation
    Example -
      . We have HTML fetcher
      . Photo fetcher...
      
    A robot.txt file created by webmaster to help certain parts of webpage restricts to be crawled
    
  C) Document Input Stream?
    The streams enables the downloaded document to be processed by multiple units.
    To avoid duplicate downloading or processing of content, it caches and runs a De-Dup service on the content
    
    it can cache small webpages of upto 64Kb in cache whereas others can be written to the disk
    
    Each worker thread has an associated DIS which it uses for streaming of documents
    
    
  D) Document De-Dup test?
    Many document are available under different URL's, example multiple host or canonical name has been used
    or mirrored on various servers.
    
    Both allows to download document miltiple times. this can be presented with de-dup test
    To perform this test, a checksum can be performed over the streamed content.
    This checksum or shas will be stored in the database
    If the checksum already exits in the databsae
    
    
      How big the checksum should be ?
        The main purpose of checksum is to store and verify the content to implement de-dup test
        The checksum has been compared in the cache or in the database
        and the document is ignored or forwarded 
        
        
        
  E) URL filters?
    It provides a customizable way to control the URL that have been extracted from the content
    
 F) DNS server?
  Before connecting the hostname from the frontier, a host to IP mapping has been done by DNS server
  We can cache the DNS mapping on  DNS server for multiple request for single host, which can imporve the performance
  
G) URl De-Dup?
  To avoid adding the duplicate URL's or already visited URl's to the fronteir queue. these provide list of URL's
  A checksum can be performed over the URL and compare with the visited set if not found, we continue with indexing the new URL
        

How much storage we need to store the URL?
  The purpose of URL de-dup is to store the information defines
  . Check sum of the file
  Considering 15 B unique URls and 4 byte to store the URL information.
    15 billion * 4 byte - 60 GB
    
 Can we use bloom filter to store if URL has been visited?
  Example:
    Bloom filters are probalistic data structure that may yeild false positives.
    A set of multiple sketches can be created to mstore the information.
    
    
7) Fault tolerant?
  Use consistent hashing to distribute the request to the API server
  Extended hashing technique can help us in replacing and adding new node from and to the the distributed design
  
  
8) Sharding?
  A) Data can deal with URL's to visit store by URL frontier
  B) URL checksum for de-dup
  c) Document checksum for de-dup
  
  A sharing of data is based on hostname by passing it to hash function and finding the associated server to process the request
  A snapshot and replica of server can be created to make it fault tolerant and resillient system
  
  

9) Crwaler traps?
  . Spam sites
  . Cloadked content
  
  These are the traps that restricts from crawling the complete or subdomain where website is hosted
  A pitfall which traps and indefiitly recurse over single domain
  
  
  A symbolic link can create a cycle to the website
  Solution?
    AOPIC (Adaptive Online page Importance computation algorithm)-  this algorithm helps mitigate common types of problems with bot
    
    . Start with N seed pages
    . before crawling a webpage associate a weight to each page with the help of frontier
    . Select a page P with the top credits. This will be selected by one of the HTTP fetcher
    . Crawl or download the page and pass the content to stream 
    . Update the credits of page to zero
    . Take 10% tax and allocate it to lambda page
    . Allocate the calulated weight to all the outbound URL's found and extracted from the streamed content
    
    
    Since Lamda page would be the page with highest credits by taking tax.
    Distrbute the credits to all the webpages store by URL frontier
    
    The bot trap page will lose more and more credits and most likely it would never be called again using the webpage
    with the good pages, we get credits from backlinks found on other pages
  










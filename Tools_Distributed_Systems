Sharding data
Optimistic and pessimistic lock
Eventual consistency (Reads will get the latest writes eventually but provides high availability)
SQL provides (ACID) whereas No Sql provides low cost and scalability

No SQl types-
	. Key/value pair (Redis)
	. Wide Column (Casandra - timeseries DB)
	. Graph based (Neo4j)
	. Document based (Mongo)

HTTP -  Request/Response 
HTTP2 - multiple request over single connection 
Web socket - Bi-Directional communcation b/w client and server

TCP/IP layers
IPv4 -
IPv6 -
Bloom filters -
Count-min sketch (Top K events) -
Paxos - Consensus over distributed hosts
Topic multi subscriber
CAS, Multithreading Concurrency, locks

Consistent hashing -
	For the scalable systems, 
	This distribution scheme is simple, intuitive, and works fine. That is, until the number of servers changes. What happens if one of the servers crashes or becomes unavailable? Keys need to be redistributed to account for the missing server, of course. The same applies if one or more new servers are added to the pool;keys need to be redistributed to include the new servers. This is true for any distribution scheme, but the problem with our simple modulo distribution is that when the number of servers changes, most hashes modulo N will change, so most keys will need to be moved to a different server. So, even if a single server is removed or added, all keys will likely need to be rehashed into a different server.
	
	So, how can this problem be solved? We need a distribution scheme that does not depend directly on the number of servers, so that, when adding or removing servers, the number of keys that need to be relocated is minimized. One such scheme—a clever, yet surprisingly simple one—is called consistent hashing, and was first described by Karger et al. at MIT in an academic paper from 1997 (according to Wikipedia).
	
Consistent Hashing is a distributed hashing scheme that operates independently of the number of servers or objects in a distributed hash table by assigning them a position on an abstract circle, or hash ring. This allows servers and objects to scale without affecting the overall system.
Imagine we mapped the hash output range onto the edge of a circle. That means that the minimum possible hash value, zero, would correspond to an angle of zero, the maximum possible value (some big integer we’ll call INT_MAX) would correspond to an angle of 2𝝅 radians (or 360 degrees), and all other hash values would linearly fit somewhere in between. So, we could take a key, compute its hash, and find out where it lies on the 

 1) Each node know address to all other nodes = Lookup(1) and Lookup Space(N)
 2) Node know's its immidiate next node like a linked list - Lookup (N) and space (1)


Example of consistent hashing: Load balancing (when a new node to be added or removed from LB) - Round robin, least connections should provide a load factor 1/n


Tools:
Cassandra
Mongo/Couch base (JSON - ACID properties)
My SQL - ACID, relationship between tables, master-slave architecture
Memcached - simple, fast, key/value storage
Redis - cluster, key/value and other Datastructures
Zookeeper - centralized configuration management tool, scalable, 
	. Monitoring system
	. Configuration management system
	. Electing lead broker (node/server which owns the topic from the follower nodes)
	. Broker messaging states are stored (to get the status/point from where consumer will read)
	. 
	
Kafka - Pub/Sub streaming application (ordered in partition)
	Assignment of task or message
	Notification / Acknoledgementn of the message
	Loadbalancing to multiple subscriber
	A heartbeat or health check mechanism
	Persistence of data/message
	Replicating the data

Proven & reliable clusture & replication mechanism
Performance
Reduce operation complexity

AWS SQS - is built on top of Kafka
	- Two topics: 
		. Queue: this stores messages to process
		. markers: this topic stores start/end markers for each message
	- Queue client/Consumers
		. Read message from queue topic
		. Write start marker to markers topic and wait for acknolwedgement
		. Commit offset to queue
		. Process the message
		. Write end offset to markers
		
	Note: The approach can be optimized by processing the messages (request/ack) in batches
	- Redelivry mechanism if message requires resend
	
	
NGINX LB - 
Solr, Elastic search - wrapper on lucene
BLOB storage - S3 AWS
Docker, Kubernetese -

Monolith V/s Microservices:
	. Monolith scales out
	. Legacy application and new features
	. N/w contract and system architecture is simpler
	. No RPC call is invloved, so faster
	
Disadvantages Monolith:
	. Bigger application and takes time to onboard new resources
	. Deployment and test are cohesive and complex
	. Too much responsibility on each server

Microservices Advantages:
	. Easier to design system as easy owns respective resources e.g. Db's
	. Scaling is easy
	. New features and resources are easyily onboarded
	. Parallel deployment is possible
	. Scale out is easier as per required services
Disadv:
	. Not easy to design system architecture
	. 
	
	
Event driven architecture:

General Flow (Not event driven)
	Client -> S0
		|_________> S1
		|_________________>S2
				  |_________________>S3
				  |_________________>S4
				  
Client sends a request to interfacing API
. Interfacing API request to other services and waits for response
. Considering the last service fails/unavailable, the data stored/processed by services in ancestral flow is stale
. Inconsistent data

  Replace with PUB / SUB model:
  	
	Client -> S0
		|=========> S1
		|=================>S2
				  |=================>S3
				  |=================>S4
. Above publishes the message in message broker
. Message broker persist the messages
. Responds the client with success response
. Any failure in downstream flow can be handled by persisting messages by brokers


Advantages:
	. easily scalable by subscribing to topics already publishing messages

Disadvantages:
	. Poor consistency for mission critical system




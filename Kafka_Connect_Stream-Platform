Kafka connect is a data streaming platform
  This platforms interfaces Kafka with other systems
  - OSS under Apache Kafka distribution
  - Simple, scalable and reliable


Input channels --------->Connect ->>>>Kafka clusture ->>>>Connect ----------->Output channels

Example-
  Stream an entire SQL data from the database into Kafka
  Stream the entire topics into HDFS for analytics and processing
  Stream the data into Elastic Clusture for secondary indexing
  (CQRS -> Command query responsibility seggragation pattern for External search and queries)


Kafka Connect is using producer and Consumer API's and that automatic
  - COTS and deeply tested connect components are available for common data sources
  - Automatic loadbalancing for distributed systems
  - No "do it yourself" i.e. no external coding required
  - Easy use existing connectors and extensible for new features

The basic responsibility of Apache Connect to 
 - Trasfer data b/w Kafka and Other systems
 - Connectors recieve data from different channels into Kafka (ingress/source)
 - Connectors sends data to external systems (egress/sink)


Types of Connect Connector:
  - aleardy developed connectors can be used [COtS]
  - New connectors can be created 
  
1. File stream 
    - Read from the last of file
    - Appends Kafka messages at end of file
    
2. JDBC
  - Polls database a new or Updated rows. Creates a Kafka message for each record modified
  - Each table has separate Topic where records are published
  - New and deleted records in table have been handed automatically

Properties (topic name
  connection.url - JDBC URL
  topic.prefix - prefix to table name for generating Kafka_Topics
  mode - mode for detecting the table changes, incrementing, timestamp, timstamp+incrementing,  bulk
  query - The custom query to run
  poll.interval.ms - the frequency poll for new data
  table.blacklist - a list of tables to ignore
  table.whitelist - a list of table to specify (one of table.whitelist or blacklist should be present)

3. HDFS
  Polls Kafka and write to HDFS
  Integration with Hive (Auto table creation and Schema evalutation with Apache Avro)
  Uses Kerberos to connect with HDFS and Hive metastore
  Plug-ins for partitioner (Kafka, fields, time, custom partitioner)

4. Elastic search
5. AWS S3
...

=================================
How do you detect new or updated rows?
  - Incrementing Column: It check for the sequence number but cannot be used for updates
  - Timestamp column - Checks a single "last_modified" column but doe not guratee all updates are read
  - Timestamp and incrementing column - Combination does achieve benefit of above
  - Custom query - 

=====================

Converters create a custom format for data to serialize the stream to Kafka
Converters are decoupled from connectors to enable reuse
  - Allow any connector to work with any serialization format
  
  DataSource ->> Custom format ---> Connectors ---> Converter --[bytes]-->Kafka clusture
                                                        ||
                                                     ========
                                                     Schema registry

Converter Data formats-
  - Key converter
  - Value Converter
  
Examples -
  AvroConverter (Best practice)
  JsonConverter
  StringConverter

----------------------
It is good to use AvroConverter with SchemaRegistry with the connectors

key.converter=io.confluent.connect.avro.AvroConverter
key.converter.schema.registry.url=<URI>

value.converter=io.confluent.connect.avro.AvroConverter
value.converter.schema.registry.url=<URI>

Benefits of Using AvroConverter:
  - A datastructure format support
  - Generation of Code files from Datastructure
  - Avro stores data efficiently
  - Typechecking of data or validation
  - Avro schema updates and evolution
  
===============================
Benefits of KafkaConnect:
  . The track record of producer and consumer connctors, for fault tolerant
 
 The method of tracking offset depends on type of Connector being used
=========================
KafkaConnect modes-
  1. Standalone mode -
    Single worker process on a single machine
    Example-
    
    connect-standalone connect-standalone.properties connector1.properties [...]
    
    The argument for properties is varards specifies properties for each Connector to be started locally
    Each connector will run in seperate worker thread locally
    
  2. Distributed mode - Multiple workers distributed across machines
  
  In this mode Kafka Connect starts on each worker node
  connect-distributed connect-distributed.properties
  
 ------------
 The Group coordination b/w distributed nodes-
   . The communication is done using group membership protocol
    (Configure working with same "group.id")
 
  . Workers distribute load within this Kafka connect "cluster"
  
  
================================  
Configuring Connectors-
  . Connectors configuration depends on the mode (standalone or distributed)
  . in distributed mode, configuration can be done only via REST API
    . Changes will be applied on restart of each worker
    . The configuration is stored in a special Kafka topic\
    . group.id - to merge connectors to make a group
    . num.partitions (default: 1)
    . replication.factor (Default: 1) - best would be 3 at least
    
    /etc/kafka/connect-distributed.properties
   
  . in standalone mode, the configuration is done via properties file
    . Configuration done using REST API are temporary change
    . /erc/kafka/connect-standalone.properties
  
  . Connectors can be added, removed or modified using REST API
    . REST request can be made to any worker


GET /connectors - Get a list of active connectors
POST /connectors - Get a new connector
GET /connectors/(string: name)/config - Get configuration information for a connector
PUT /connectors/(string: name)/config - Create a new connector or update the existing config


===================================
SMT (Single message transforms) - a light weight operations performed on message
  The ability to transform a message at a time
  - Configured at the connector-level
  - Applied at message key or value level
  
Few transformations Example-
 InsertField: insert a field using attributes from the message metadata or from configured static value
 ReplaceField: rename fields, or apply filters(whitelist, blacklist)
 ValueToKey - Replace the Key with a new keyformed from a subset of fields in value payload
 TimestampRouter - Update the Topic field as a function of Original Topic value and timestamp

=================================

Administration of Kafka - 
  * Different types of installers - tar, archive, zip, RPM or docker images
  * Brokers are built as backward compatible
  * In clusture rolling upgrade is possible
  
  
-- Topics have been automatically created when first used
  Deafult (Single replication and Single Partition)

in production the auto.create.topics.enable=false. the topics should not be generated automatically

Instead of autogeneration, create topics manually-

kafka-topics --zookeeper HOST:PORT --create --topics TOPIC_NAME \
  --partition 6 --replication-factor 3
  
kafka-topics --zookeeper HOST:PORT --alter --topic TOPIC_NAME \
  --partitions 30
  
 kafka-topics --zookeeper HOST:PORT --delete --topic TOPIC_NAME
 // Stop all producers/Consumers before deleting a topic
 // All Brokers must be running for a delete to be successful


=====================================
Kafka Log management-
























/


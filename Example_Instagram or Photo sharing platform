Instagram?
  Instagram is a social networking service which enables its users to upload and share their photos and videos with other users.
 Instagram users can choose to share either publicly or privately.
 Anything shared publically can be seen by any other user, whereas privately shared contents can only be accessible by a specified set of people.


1) Functional requirements?
  . Upload/Download/View the photos' or videos
  . User can perform search based on photos or videos
  . User can connect with users
  . A feed from others users to be displayed
  
2) Non-functional requirement?
  . Availability
  . Low latency for file upload and news feed generation
  . Eventual consistency
  . The system should be reliable that any photo's/media upload should never be lost
  
 3) Design considerations - read/write heavy?
  The systems should be ready heavy as news feed will be shared will connections
  . User can upload as many as photos
 
4) Estimations:
  Assuming it has 1M active request per day
  1M photos have been shared every day i.e. 12 photos per second
  Average size of photos is 200 Kb
  
  
  A) Traffic estimations?
    
  B) Space estimations?
    1M * 200 Kb = 200 GB per day
    
    For 10 years
      200GB * 365 (days) * 10 = 712 TB 
    
  c) Bandwidth estimations?
  
  d) Memory estimations?
  


5) API definition?

6) Database design?
  a) Photo table - stores file path for distributed file store Amazon S3 and Id (Scalable - NoSql)
  b) User table - Relationship (MySQL or Columar database - casandara)
  c) Follower table - to store all the connections (MySQL or Columar database - casandara)

7) High level design?
  a) Upload a photo by user
  b) View/Search photo 
  
  
8) Component design?
  If we assume that a web server can have a maximum of 500 connections at any time, this would mean it can’t have more than 500 concurrent uploads or reads.
 To handle this bottleneck we can split reads and writes into separate services.
 We will have dedicated servers for reads and different servers for writes to ensure that uploads don’t hog the system.


Separating photos’ read and write requests will also allow us to scale and optimize each of these operations independently.

9) Redundancy and reliability?
  Losing files is not an option for our service.
 Therefore, we will store multiple copies of each file, such that if one storage server dies, we can retrieve the photo from the other copy present on a different storage server.


Creating redundancy in a system can remove single points of failure and provide a backup or spare functionality if needed in a crisis.
 For example, if there are two instances of the same service running in production, and one fails or degrades, the system can failover to the healthy copy.
 Failover can happen automatically or require manual intervention.


What are the different issues with this partitioning scheme?

How would we handle hot users? Several people follow such hot users, and a lot of other people sees any photo they upload.

Some users will have a lot of photos compared to others, thus making a non-uniform distribution of storage.

What if we cannot store all pictures of a user on one shard? If we distribute photos of a user onto multiple shards, will it cause higher latencies?
Storing all photos of a user on one shard can cause issues like unavailability of all of the user’s data if that shard is down or higher latency if it is serving high load etc.

Wouldn’t this key generating DB be a single point of failure? Yes, it will be.
 A workaround for that could be, we can define two such databases, with one generating even numbered IDs and the other odd numbered.
 For MySQL following script can define such sequences:


KeyGeneratingServer1:
auto-increment-increment = 2
auto-increment-offset = 1
​
KeyGeneratingServer2:
auto-increment-increment = 2
auto-increment-offset = 2
We can put a load balancer in front of both of these databases to round robin between them and to deal with downtime.
 Both these servers could be out of sync with one generating more keys than the other, but this will not cause any issue in our system.
 We can extend this design by defining separate ID tables for Users, Photo-Comments or other objects present in our system.


8) Cache and Load balancing
Our service would need a massive-scale photo delivery system to serve the globally distributed users.
 Our service should push its content closer to the user using a large number of geographically distributed photo cache servers and use CDNs (for details see Caching).


We can introduce a cache for metadata servers to cache hot database rows.
 We can use Memcache to cache the data and Application servers before hitting database can quickly check if the cache has desired rows.
 Least Recently Used (LRU) can be a reasonable cache eviction policy for our system.
 Under this policy, we discard the least recently viewed row first.


How can we build more intelligent cache? If we go with 80-20 rule, i.
e.
, 20% of daily read volume for photos is generating 80% of traffic which means that certain photos are so popular that the majority of people reads them.
 This dictates we can try caching 20% of daily read volume of photos and metadata.










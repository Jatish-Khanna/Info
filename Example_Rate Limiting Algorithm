A service which throttles based on the number of request

Imagine we're a service providing limited number of free reqeust per user or a service which can handle limited number of request
To handle this problem, we need some kind of throttling mechanism 
which allows number of request that our service can serve

1) Rate limiter?
A rate limiter at high level  limits the number of events an entity can perform in a particular time window. Example
  . A user can send only one message per second
  . A user is allowed to fail three transactions per day
  . A single IP can perform twenty request per day
  
  A rate limiter caps how many request a server can serve in a specific time interval
  
2) Why do we need rate limiting?
  It protects services using abusive request behavior at application layer like
    . DDoS
    . Brute force attemps for password
    . Brute force credit card transactions
    . To reduce revenue loss and prevent the infrastructure costs
    . To stop spam and online harassment
    
 3) Requirements?
    Functional requirements?
      . limit the number of request an entity has send to be served by the system
      . The request are distributed so the algorithm should work as one one combined system
      
    Non-functional requirements?
      . The systems should be highly available.
      . The performance of system should be consistent
      
 4) Design considerations?
  Rate limit: It is service at which the rate and speed at which customers can access API's 
  Throttling: is the process of controlling the usage of API's  
    When throttiling limit is crossed, the server return 429, "Too many requests"

5) High level design? - types of rate limiting strategies
  . Hard throttling: The number of API request cannot be increased after certain limit
  . Soft throttling: This is rate limiting in which API request limits are defined to certain %age
  
  
  . Elastic or Dynamic throttling: The number of request can go beyond the threshold if resources are available
  
6) Low level design?
  A) Fixed window algorithm?
    The time window is considered from start to end of the time-unit
    Example:
      A period would be considered as 0-60 sec for a minute
    
  How it works?
    Assume we want to limit the number of request per user. For each user we would count representing how many request the user 
    has made and a timestamp when the request count has been started
    
    . Keep the entry in the hashtable, a user ID as the key 
    . Value: Epoch time and count the number of request
    
    Key: userID
    value: {count, Epoch time}
    
  
  Steps: a request has been submitted by user to be served
    a) If the user Id is not present in the HashTable, insert it into the hashtable
      . Increase the count = 1 and Epoch time = to current system time
      
    b) If the userId is present in the HashTable, if current time - start time >= allowed limit
      . Reset the count to = 1 and start time as current system time
      
    c) if current time is with in the time window
      .  if count < number of allowed request by the system
        - serve the request and increment the counter
      . Reject the request

Issue with Fixed window algorithm, as the counter has been reset every minute at fixed interval?
  a) A system will serve twice the number of request each minute
   Example:
    A user send a limited request at the end of time frame
    and again sends a limited request at start of next interval
    
    With in rolling window it is consideres as double the number of request allowed by system

  b) Atomicity?
    In distributed environment, constant read/write behavior can cause race conditions
    In case: a request has been served by API server at the same time the next set of request has been loadbalanced to other API server
      Both the server are accessing the stale read count which allows them to serve the request
      
    Solution:
      A Redis to be used a distributed key/store where atomicity can be resolved for the duration of read/update operation
      or some other mechnaism to lock the data which resolves issues with atomiticty
      
      How much memory we need to store user data?
        . User ID takes 8 bytes to store the request
        . A 2 byte count
        . A 4 byte Epoch time (where as 2 byte is sufficient to store only minute and second part)
          8 + 2 + 2 = 12 byte per user
          
          Assume our hashtable has an overhead per each record i.e. 20 bytes
          ( 12 + 20 ) * 1 Million users = 32 MB
  
      Note: if we have a 4 byte lock to resolve atomicity issue than required space is 36 MB
  
  ALthough, we can fit all this on one server but the number of request cannot be handled by One server.
  A rate limit of 3 request per second does mean 3 million request per second (considering 1Million users)
  
  All the data will be stored on Redis clusture which improves the performance
  
   B) Rolling window algorithm?
    A time window is considered as from when the request has been made plus the time window allowed
    
    . A time stamp of each request will be stored in the hashtable as value

    Example:
      Key: User ID
      Value: Sorted Set<unix time,...>
      
     Steps followed by the rate limiter?
      a) If a user ID doesnot exist in the HashTable it inserts an entry into table
      b) if User Id is present in the table-
        . Iterate over all the sorted set unix time stamp values
        . If the timestamp is out of window, remove the item from sorted set
        . Calculate the size of SortedSet, if larger than the number of request allowed - discard the request
      c) Accept the request of request count is with in the bounds of number request allowed for the system
      
      
   How much memory do we need to store per user?
    Assume we allow 500 reqeust per hour 
    a user Id takes 8 byte
    Epoch time 4 byte
    20 byte per sorted set overhead
    20 byte overhead for hashmap
    
    8 + (4 + 20 (sorted set overhead to store timestamps)) + 20 byte for hashtable = 12 Kb
    
    Here are reserving 20 bytes overhead per element.
 In a sorted set, we can assume that we need at least two pointers to maintain order among elements â€” one pointer to the previous element and one to the next element.
 On a 64bit machine, each pointer will cost 8 bytes.
 So we will need 16 bytes for pointers.
 We added an extra word (4 bytes) for storing other overhead.

For 1 Million users:
  1 Million * 12 KB = 12 GB
  
  Note: when compared with the fixed window (36 MB), it (12 GB) take lot of extra space 
  
  Solution: Combine benefits of both the algorithms
  
  
  
  C) Sliding window with counter?
    . We track of request count for each user using multiple fixed windows
      Example: 
        If we have an hourly rate limit algorithm-
        . Keep an count on each minute 
        . Sum of counter in the past hours when we receive new reqeust
      
      For 500 request per hour with an additional limit of 10 request per minute
      i.e. A server can serve no more than 500 request per hour for a user 
      also no more than 10 request per minute
      
      

  How it works?
    a) user send a request and ID is not present in the Redis or HashTable, a new ID will be inserted in the hashtable
      
    b) If user ID is present in the hashtable
      . A request counter has been incremented
      . A request serve time window will be incremented to allowed time limit
      
      
      How much memory do we need to store?
        . 8 byte for each user ID
        . 4 byte for the Epoch time
        . The count will take 2 bytes
        . 20 Byte overhead for Hashtable
        . 20 byte overhead for redis Hash
        
        Since we keep count for each minute i.e. 60 entries per hour
        
        For 500 request per hour rate limiting algorithm will take?
          8 (user ID) + (2 (counter) + 4 (epoch time) + 20 (redis hash overhead)) * 60 entries + 20 (HashTable over head) = 1.6 Kb

  Tracking for 1 million users, total memory need is
    1 Million * 1.6 Kb = 1.6 Gb
    
    Note: it take 86% less memory than the sliding window algorithm
    
    
7) Sharding and caching?
  Shard the request per user Id, for fault tolerant and replication we use consistent hashing
  
  Example- We want to rate limit URL shortner API to create no more than 100 URL per hour
  Assuming we are using hashbased partitioning, we can rate limit each partition to allow user to create no more than three URLs per sec
  
  Note:
    Our system can get huge benefit from caching the active users
    Before sending the request to backend server, if we check in memory cache server will imporve the performance
    A write-back cache would suit the - where we update and read from the cache not from backend system
    
    The reads can always hit the cache first, which will have huge performance improvements when user hits its limit
    
    
8) Should we rate limit by IP or by user?
  A) IP -
      The biggest challenge is when multiple users share the sinlge IP to direct traffic to Internet
      With this, one bad user may throttle down for all the other users
    
    As with IPv6 available to  a hacker  from even one computer, it make make server run out of memory (in tern a DDoS)
    
  B) Rate limiting by user-
    Once the user has been authenticated, the user will be provided with apiDevKey or token,
    this token will be passed with each request
    - This token will be key against which we rate limit
    
    What if we need to rate limit on anonymous user?
      An attacker can perform DoS again the login servive or the API by sending the reqeuest.
      
      
  C) Hybrid?
    A right approach would be to do it by IP based and user based rate limiting
      This will require more cache entries and with more details , hence requiring more memory usage











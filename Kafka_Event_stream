kafka-
  A database that is storing, processing, and reacting infinte streams of data - a Pub/Sub stream data platform
  
  1. Like Hadoop - but Kafka stores continuous stream 
    whereas Hadoop stores big-batch of the files and process them on scheduled batches
    
  2. Enterprise Messaging - It is unified,elastic, cloud ready and modelled on distributed foundation systems
      
  3. Evolution of ETL and intergration-
    Any channels of input can source data/event to Kafka
    
    
    Example -
      A self driving car - ingress data from different cameras and sensors
       egress - the predictive actions
       
      Fraud detection systems
      E-com recommendation systems
      Log monitoring
      
      
  Kafka considers each action as event and such we have stream of events
  
       
-----------------
Application to Cloud-
  Rather than having point to point connections b/w the public and on-premise cloud
  we have a bridge as a Kafka that publishes the streams on both sides
  
  . This helps in maintaining
  . Sync data
  . All this is real-time as part of event stream
  . Scalable with Kafka clustures
  . Huge storage as if connection is down or backpressure to be implemented
  
------------------
Apache kafka

Kafka Connect-
  ingest from RDBMS, Mainframe, IoT ...
  Consumers as Hadoop, NoSQL, data stores, Object storages like S3
  Middleware integration b/w different API's
  
Kafka Streams-
  For embedded stream processing in applications like another library

Kafka Stream Interactive Query - 
  to retrieve state from Kafka i.e. monitoring
  
 ------------------
  Confluent OpenSS Kafka

Schema Registry - for data governance/quality
Connectors - drivers for different systems
REST proxy - 
------------------
Confluent Enterprise 

Control Center - for management, Graphical management and alerting
Replicator - Multi-data center replication between Kafka clustures
Auto databalancer - shifts data to create an even workload across the Kafka
Client Library: JMS messaging serive
And Enterprise support -



==================================
Components when discuss Kafka -

  Kafka clusture
  Producers - Serilalized 
  Consumers
  Brokers
  Message
  Topics - Named pipes/queues - Ordered collection []
  Zookeeper

Message in the topic are in the form of byte-array but 
  the typing contract is required which defines the format of message inside each topic
  
  - Each topic maintains the ordered collection of messages
  - Many producers can write to topic
  - Multiple consumers can read from topic
  
Kafka is a scalable middleware integration system, considering that problems to deal with-
 *) One broker may fail and all the data stored or topics may lost
 *) Huge load on a topic to be handled or Too much data that cannot be stored on one machine
 
 
 Solution:
 Partitioning Data ingestion:
 . Each partition contains the subset of the "Topic's messages"
 . Each partition is an "ordered" like queue, immutable log of messages
 
Partitions are distributed across brokers
*) Message key determines which partition a message is assigned to [Hashes the Key]
  - Consumer can override the Key at the time of Ingestion


==============================
Kafka Producers?
  Are the Channels/Programs that writes messages to Kafka Clusture/Broker/Partition
  - this can be done using the API's
  - Producers can be written in any Language as Drivers are available
  - native language of Kafka is Java
  - REST proxy provides that can also be used to send messasges
  - A CLI is available for debugging and testing purpose
  
  Partitioning Strategy? or I say it Loadbalancing b/w the Kafka paritions
    How Producer decides to choose Kafka Topic distributed amond Kafka Brokers in clusture
    
  How This is done?
    * Hash the key of Kafka message
    * Select the respective Parition for the hash
    * If Keys are not specified than default (Round robin fashion)
    
  Challenges or  Virtue?
    - The above partitioning strategy doesn't maintain the Order of Kafka messages
    - If Ordering is important than-
      Override the Partitioning strategy or Load balancing
      
      
    Note: By default, Each partition maintains the Order but Topics (doesn't maintain)
    
 ==================================
 
  Kafka Brokers?
    - Brokers receive messages and store messages when they are sent by the Producers
    - A Kafka cluster has multiple brokers
     - Each Broker has multiple partitions
     
  Note: Kafka is not a database 
  
  Kafka runs faster as the contract b/w Producer and Consumer is simple
  
 A typical architecture looks like---
  . Each Message in a Topic has been distributed across Partitions in Multiple brokers
  . A broker has multiple partitions 
  . Each partition has log files on the Broker disk to which it belongs to
    Properties of Logs?
      . Append only
      . Written data is immutable
      . Rolling file when size grows or based on the retention policy
            
  . Each message in the log is identified by its Offset'
    - Example a sequence number or monotonically increasing value
    
  . Each Broker has retention policy to manage log file growth (Default - 7 days)
  
  
  Benefit-
    . Each Consumer maintains it ReadOnly Offset to read the value from the logs files
    
 =============================
 
 Kafka Message?
  A segment which has been transferred by Broker using a partition
  This would be read and processed by Consumer later on.

Message Metadata?
  Key/value pair
  Offset
  Timestamp
  CompressionType
  
  
  |Offset.....|MessageSize.....|CRC...Magicbyte...Attributes...Timestamp....|KeyLength...KeyContent..ValueLength..ValueContent|
  
===============================
    12 Bytes                   =============================================
                                          6 or 14 Byte Header               ==================================================
                                                                                    Key/value pair(Bytes varies)

Magic Byte - Indicates the timestamp is present, which impacts the consumer


Fault Tolerant with Brokers?
  Replication - A typical replication factor is 3
    A lead broker and two follower that Keep standby copies and sync with the leader
    
  The Failover process has been handled by Kafka infrastructure does mean
  developer should not build that aspect in code
  

=========================================================

Kafka Consumers?
  A client program that polls the data from the Kafka Topics/partitions
  - A consumer subscribes to Kafka Topics in the clusture
  - Each consumer maintains its Offest irrespective of the Other Consumers
  - As each message written by Kafka on logs with a retention Policy
   . Consumers can change the offset to re-read messages
   
 - The consumer Offset is stored in a Kafka special topic, for the partition
 - A Consumer can be written in multiple languages
 - Also a CLI consumer is available to test, bebug Kafka setup
 
 
 * Deafult(Each consumer will receive all the message from the Topic)
 * Different Consumer can subscribe to same Topic
 * Consumer can create scalining group i.e. Consumer Groups
  where each Consumer is assinged to a subset of partitions of a topic
 
 
** How to handle Backpressure?
  * Using Consumer Groups - Dynamically adds or/ remove Consumers
  * Kafka store the messages as per the retention policy


=======================================

Zookeeper?
  A project out of Hadoop
  Centralized service that is highly reliable distributed Coordination to synchronize resources
  
  * Open source project
  * distrubuted coordiantion service
  * synchronization b/w clients
  * Maintains the configuration information
  
 *The Service usually consists of three or five nodes in production quorum


How does Kafka relates to Zookeeper?
  - Brokers use Zookeeper for Clusture management
  - Failure detection and recovery
  - Leader election
  - ACLs
  - Maintaining metadata

====================================

What is more in Kafka - than other middleware integration systems?
  * Publisher/Subscriber messaging system
  * Falut tolerant
  * Data storage as per retention policy
  * Multip consumer (can read the same message) e.g. a slow processing queue, and real-time stream
    consumes messages at different rate
  
  * Each Consumer can maintain offset
  * Highly available and durable
    - Messages are replicated on multiple machines
    - Clusture with help of Zookeeper can handle failures
    
  * Scalable [desinged for scale out] - Each clusture and broker can process huge number of messages
  * Realtime and Batch Consumption

  * Decouple producers and consumers
  * Multi
    . brokers
    . topics
    . Consumers [Pull architecture, each consumers polls a message]
    . Consumer groups [Manages the heartbeat with each consumer]
    
    
  * All the messages are processed by brokers as log messages
  * Utilizes OS page cache to hold recent produced data which can be consumed
  
  * High speed transfer - (A zero copy transfer)
    Data is not written by JVM on heap instead directly transferred to n/w 
    using OS page cache once message has been received from broker
  
  ===================================================
  A sample Kafka commands or CLI:
  
    1. Start Kafka
      
    2. Produce messages
       kafka-console-producer --broker-list HOSTNAME:PORT --topic TOPIC_NAME
       ...
       ^+d to stop typing and send the data
    
    3. Consumer messages
  
      kafka-console-consumer \
    --bootstrap-server HOSTNAME:PORT \
    --from-beginning \
    --topic TOPIC_NAME

      ^+c for stop reading the messages
    
    4. Administer Kafka using Zookeeper
    
     zookeeper-shell NAME
        ls /
      [schema_registry, cluster, controller_epoch, controller, brokers, zookeeper, admin,
      isr_change_notification, consumers, config]


===================================================
  Kafka Logs - a fault tolerant feature with Partition logs in Kafka?
    1. Commit logs-
      Persist the messages sent by consumers on the disk
      This provides the privilege to iterate using Offset
      
      - Producer can write to single partition (provides sequenctional access using commit logs)  
      - Prodcuer can write to multiple partitions
  
  
   2. Offset Logs- That stores index offset for each consumers
   
   3. Timestamp to file offset


============================================

Kafka Replication - fault tolerant?
  * A single partition has been replicated across multiple brokers in clusture
  * Three is the replication factor in production
  
  * Where 1/ 3 nodes is Leader and others are followers
  * All writes are handled by leaders, which has been propagated to followers

  * ISR's (in-sync replicas) which replaces leaders in case of failures
  
 
 Note: One broker is designated as Controller in the entire clusture
  - This broker detects failure and spawn new instances via Zookeeper
  - Selecting new leader from the ISR list
  - Stores leader information and ISR list in Zookeeper
  - Propagate changes to all the brokers (e.g. ISR list or new leader elected)
 
 If Controller failed, then one of other broker is elected as new Controller
 
 
* All reads/writes are done through leaders of the topic paritions

* A single partition has been consumed by single consumer in the Consumer group

========================
Partition strategy for assignment and re-assignment with the Consumer in ConsumerGroups?
  1. Range partition - 
  2. round robin
  
  
  Note: All messages with the same key will got to same Consumer, until there is change due to failure or addition
  
=========================================


Create a Producer ?
  - Extend KafkaProducer class 
  - KafkaProducer class is threadsafe
  - A producer instance will be shared among multiple threads instead of creating multiple instances
  - Use Properties class to configure Producer

===Common properties====
   
   bootstrap.servers: host/port pairs to establish connection with KafkaClusture
   
   // StringSerializer, ByteArraySerializer, IntegerSerializer...
   
   key.serializer - Class to serialize the key, implements Serializable interface
   value.serializer - Class to serialize the value, ""
   
   compression.type -The data compression like none, gzip, lz4, snappy. The compression is done on batch of messages

   acks: The no. of acknoledgements the producer requires the leader to have before 
    considering the request complete.
      
     acks=0 No ACK required by the producer  // Unreliable delivery
     acks=1 Unitl leader has committed the log to Commit log file // 
     acks=All for all the nodes to replicate the change 


  =====================================
  send() - a non-blocking call to send the messages to partition
    returns a Future
    
  
  
import java.util.Properties;


import org.apache.kafka.clients.producer.Producer;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;

class ProducerKafkaExample {
  
  
	public static void main(String []args) {
      String topicName = args[0].toString();
      
      // create instance for properties to access producer configs   
      Properties properties = new Properties();
      
      // Kafka server
      properties.put("bootstrap.servers", "localhost:8081");
      
      // Only leader should ACK the message     
      properties.put("acks", "1");
      
      // If the request fails, the producer can automatically retry,
      properties.put("retries", 0);
      
	  /*
	  * Batch Configuration
	  * High throughput - 
	  *   large batch.size and linger.ms
	  * low latency -
	  *   small batch.size and linger.ms
	  */
	  
      // Message Batch size - 16KB
      properties.put("batch.size", 16384);
      
      // Time to wait for messages to batch together (Deafault: 0 send immidiatly) 
      properties.put("linger.ms", 1);
      
      //The buffer.memory controls the total amount of memory available to the producer for buffering
	  // Default 32 MB
      properties.put("buffer.memory", 33554432);
      
	  // Key and value serializer
      properties.put("key.serializer", 
         "org.apache.kafka.common.serialization.StringSerializer");
         
      properties.put("value.serializer", 
         "org.apache.kafka.common.serialization.StringSerializer");
      
      // Create an Object using the configuration
      Producer<String, String> producer = new KafkaProducer<>(properties);
            
      for(int i = 0; i < 10; i++) {
      // can be replaced with Lamda or Future/Callback
        producer.send(new ProducerRecord<String, String>(topicName, Integer.toString(i), Integer.toString(i)));
        
        System.out.println("Message sent successfully”);
		}
      producer.close();
		
   }
	
}



----------------------------------------------
 REST proxy - Producer (to allow any type of client to use Kafka)
 
  - the REST proxy allows to perform HTTP actions on Kafka clusture
  - REST calls are translated to native Kafka calls
  - Key/Value JSON should be encoded in Base64 
  
  - POST is used to send the data
  - GET to retrieve the data from the Kafka clusture


========================================


import java.util.Properties;
import java.util.Arrays;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.ConsumerRecord;

class KafkaConsumerExample {


	public static void main(String []args) {
      String topicName = args[0].toString();
      
      // create instance for properties to access consumer configs   
      Properties properties = new Properties();
      
      // Kafka server - should list multiple servers in production
      // Three brokers at least in production and should be up and running
      properties.put("bootstrap.servers", "localhost:8081");
	  
	  // To apply the magic of Consumer group
      properties.put("group.id", "myFirstGroupId");
      // To notify clusture the Offset until the consumer has read the messages
      properties.put("enable.auto.commit", "true");
      // Default: 5s i.e. 5000Sec
      properties.put("auto.commit.interval.ms", "1000");
      // 
      properties.put("session.timeout.ms", "30000");
      
 
      
	  // Key and value deserializer 
      properties.put("key.deserializer", 
         "org.apache.kafka.common.serialization.StringSerializer");
         
      properties.put("value.deserializer", 
         "org.apache.kafka.common.serialization.StringSerializer");
      
      KafkaConsumer<String, String> consumer = new KafkaConsumer
         <>(properties);
		 
	 //Kafka Consumer subscribes list of topics here.
	 // Multiple topics can be subscribed by single Consumer
      consumer.subscribe(Arrays.asList(topicName))
      
      //print the topic name
      System.out.println("Subscribed to topic " + topicName);
      int i = 0;
      ConsumerRecords<String, String> records;
    try{
    while (true) {
      // Number of ms the call should blocked if no messagess are found
      // Here it is 100 ms
      
         records = consumer.poll(100);
         for (ConsumerRecord<String, String> record : records)
         
         // print the offset,key and value for the consumer record.
         System.out.println("Offset: "+ record.offset());
		 System.out.println("Key: "+record.key() + " Value: "+ record.value());
      }	 
        }finally {
		consumer.close();
	}
   }
	
}

// defines the size of data per poll()
// max.partition.fetch.bytes : default 1048576 - 1MB


// Consumer is not thread-safe

Note: Bigger messages than architecture should be reconsidered,
  Ideally, the blobs should be stored on Distributed FileSystem and reference should be passed using Kafka
---------------------------
============================


Consumer Offset in Kafka-
	auto.offset.reset - defines the action if Consumer offset is not available

	* earliest - The earliest available [begining]
	* latest [default]- resets to latest offset
	* none - Exception as no Offset is defined by Consumer

	Posibilities when Consumer's Offset is not available?
		. When a new ConsumerGroup has been initialized
		. Consumer offset is smaller than smallest offset
		. Consumer offset is greater than largest offset
		

The KafkaConsumer API provides ability to customize the Offset for each Consumer

 * View Offset - 
 		position(TopicPartition) - Offset of the next record to be retrieved
		offsetsForTimes(Map<TopicPartition, Long>) - provides offsets for the given partitions by timestamp
 
 * Change Offset (methods)
		- seekToBeginning(Collection<TopicPartition>) - seeks Earliest offset of each partition
		- seekToEnd(Collection<TopicPartition>) - seeks to latest offset for all the partitions
		- seek(TopicPartition, offset) - seeks to Offset for specified partition
		
		
	assignment() -> Returns Collection(list) of all partitions Consumer is subscribed to
		
		
======================================

Consumer Liveness?
  Heartbeat is a part of separate thread(background) 
New defaults?
	- session.timeout.ms - Default 10sec
	- max.poll.records - defaults 500
	// When poll thread is hung
	- max.poll.interval.ms - Defaults 5min (maximum allowable intervals b/w calls to pill)
		

==================
Consumer Rebalancing?
  - A consumer leaves the ConsumerGroup(either failed or graful shutdown)
  - A new Consumer Joins the group
  - A Consumer changes it subscription
  - The Consumer Group notice a change to the Topic metadata 
    like increase/decrease/failure of Topic partition
		
Note: During rebalancing the consumption has been paused

What will happen after rebalancing?
  - Allows to dynamically modify the consumer groups without impacting the flow
    like adding/removing a new consumer to the ConsumerGroup
  - To handle failure conditions with Consumer Failure
  
 Issues and mitigation while Rebalancing?
   - If a consumer maintains internal state of Offset then rebalancing will cause problem
   - A paused partition will no longer be paused
   
 Option#2 while rebalancing ?
   Provide a ConsumerRebalanceListener, while subscribing to Topic
   - implement onPartitionsRevoked and onPartitionsAssigned method
   
 // This means upgrade the Consumers to streams
 
=====================

Manually Committing Offsets?
  Consumer offsets are committed in background to Controlled Broker
  enable.auto.commit - true 
  // This autocommit happens when calling poll() method - default 5 sec
		
Issue?
  - A rebalance happens after Two seconds of Last autocommit (default 5 sec)
   . New consumer reassigned during rebalancing will start consuming messages from
     latest committed offset position
   . The messages were processed two seconds will processed again
   
   [High latency, and High load systems will need to process those Hundereds and thousands messages again]
 
 
Override the Commit funcationality to Manual?
  1. Disable the enable.auto.commit=false 
  
  2. commitSync() -> 
    call it after poll(), and then process the messages i.e. At most once delivery
    The method call commitSync() is blocked until it succeeds and retry as long as 
      it doesn't receives a fatal error
      
    Issue-
    All the messages retrieved by poll() should be processed properly else Consumer may miss messages
    
  
  3. commitAsync() -
    This method is immediately returned after the call
    Parameterized callback i.e. optional and this will be triggered when Broker reponds
    Throughput is high as Consumer can process next batch 
    
    The Issue - 
    Noticed if the commit failed
  
  Note: Use seek() methods to start processing messages from offset
  
  ==========================================
Kafka Offsets in a special Topic?
  The topic where Consumer offsets have beens stored asre __consumer_offsets
  
================================

Default(Symantic Partitioning scheme): 
 - Kafka will hash the key and map to specific partition
 - It is certain that same key will be hashed to same partition
 - If Key is null, then default partition is used
   The record will be sent to random partition(round-robin algorithm)
   
 
Implement the Partitioner interface to create custom partition?
 Methods:
 - configure
  close  
  partition - takes Topic, key, serialized key, value, serialized value and Clusture metadata
    [0-indexed partition scheme]
   
   
		
Method#2 -
  ProducerRecord<String,String> record = new ProducerRecord<>(topicName, partitionNumber, key, value)
		
		
		
		
		
		
		
		
		
		























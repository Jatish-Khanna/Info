Artificial Intelligence - 
  the theory and development of computer systems able to perform tasks normally requiring human intelligence, 
    such as visual perception, speech recognition, decision-making, and translation between languages.

computer vision is concerned with the theory and technology for building 
   artificial systems that obtain information from images or multi-dimensional data.

Speech Recognition
NLP
Machine Learning
Robotics
Planning, Scheduling and Optimization

Machine learning is the scientific study of algorithms and statistical models that 
  computer systems use to effectively perform a specific task without using explicit instructions, 
  relying on patterns and inference instead. 
  It is seen as a subset of artificial intelligence

Types of Machine Learning
  1) Supervised and Semi-supervised
   Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs
  
    Y(Output) - predicted values, dependent variables
    X(input) - predictors, independent variables
    
    Example - Regression algo. 
  
  2) Unsupervised
    Unsupervised learning algorithms take a set of data that contains only inputs, and find structure in the data, 
    like grouping or clustering of data points. The algorithms therefore learn from test data that has not been labeled, 
    classified or categorized. Instead of responding to feedback, unsupervised learning algorithms identify commonalities 
    in the data and react based on the presence or absence of such commonalities in each new piece of data. 
  
  Example - Classification
  
  3) Reinforcement learning
     Reinforcement learning is an area of machine learning concerned with how software agents 
     ought to take actions in an environment so as to maximize some notion of cumulative reward.


Scatter Plot - a graph in which the values of two variables are plotted along two axes, 
            the pattern of the resulting points revealing any correlation present.

Regression 
    Linear Regression is a machine learning algorithm based on supervised learning. It performs a regression task. 
    Regression models a target prediction value based on independent variables. 
    It is mostly used for finding out the relationship between variables and forecasting.

 f(x) = b0 + b1x1 + e0
  Regression coefficients - b0 and b1
  x1 independent variables (predictors)
  f(x) - y predicted value
  e: error term

When the number of features becomes large then 
the closed form expression may become inefficient 
because it involves computation of matrix inverse and matrix multiplication which is expensive. 

In such scenarios where the number of features is large, 
we may use Gradient Descent to estimate the coefficients based on least squares. 

----------------------------------------------

How to calculate the Accuracy of Model- 

R^2 - (in simple linear regression) : coefficient of determination.
  This is used to determine, how accurate the regression is
  R^ values lies b/w 0(in accurate) - 1(accurate)

R-Sqaure (R^2) = Sum of squared regression / Total sum of square

Sum of squred errors [SSE] = sum(YActual - YPredicted)^2

Sum of squared regression [SSR] = sum(Ypred - Ymean)^2
  the total sum of squared deviation of the predicted values from the population mean

Total sum of square[SST] = SSR + SSE = sum(yActual - Ymean) ^ 2
  
-------------------------------------

Multiple Linear regression - When we have more than one predictor variable
  For example:
    The volume of a tree trunk might be dependent on its height and girth. 
    The price of a house might be dependent on the number of bedrooms, the built-up area of the plot, the age of the house etc.
    The height of a child might be dependent on age, weight, heights of the parents etc.

y = b0 + b1x1 + b2x2 ... bnxn + E (epsilon)

---------------------------------------
MultiCollinearity
  In a multiple regression model where two or more predictor variables are involved, 
  it is possible that one predictor can be linearly predicted from the others, with a substantial degree of accuracy. 
  In such a situation, the predictors are said to be highly correlated.
  In statistics, this phenomenon is called multicollinearity, or in other words collinearity

Assumptions of Linear Regression -
  There should be no MultiCollinearity between independent variables
  The variables are suggested to be linearly dependent if the correlation values are close to 1. 
  Multicollinearity - impacts erratic results in the accuracy  
  
VIF - Variance Inflation factor
  variance inflation factor(VIF) to determine if the predictor variables are independent of each other.
  It is used to measure Multocollinearity. 
  If VIF is between 5 - 10 the variables are said to be multi collinear
  
  1 / (1 - Ri ^ 2)

Adjusted R^2 (R-Sqaure) - 
  Using the least squares method we try to establish a best fit linear regression model with minimum error. 
  For a linear regression model every additional predictor variable tends to minimize the error of the model.
  As a result the R2 value will never decrease for any number of additional predictor variables being included in the model.
  
use of an additional statistic known as adjusted R2 is suggested. 
The adjusted R2 takes into account the number of predictor variables included in the regression model.

Ra^2 = 1 - ((SSE/ (n-k-1))/ (SST / (n - 1)))

whereas 

R^2 = 1 - (SSE)/ (SST)

Where n is the number of observations and k is the number of number of predictor variables in the model. 

--------------------------------------------

Feature Engineering - Cleaning the data, transforming the data, data distribution, ...
  To select the predictor variables that impacts the performance, accuracy of a Machine Learning model

Continuous variable: e.g. Speed, temprature
Categorical variables: Marital Status, Education

---------------------------------------------
Typical process involved in Machine learning -

1. Historical data:
2) Feature Engineering - PCA
3) Split data - Train and Test
4) Train the Model using train data - (Model gives accuracy or R^2 value)
5) Validate the model using test data 
6) Deploy in production

---------------------------------------------

Note: main of regression is to have lower error (verified using different methods like RMSE, R^2)

Root Mean Squared Error [RMSE]-
  Sqaure root of Mean squared error  
High RMSE : Model performance on train and test data is poor - Under fit model
Low RMSE and High R^2 : Model is good but High RMSE (poor for test data) - Over fitting
 
---------------------------------------------

Classification -
   2 kinds of predictions - a category (e.g. human/non-human in case of the self-driven cars) and a quantity
   predicting a quantity, the models built are referred to as regression models
   when predicting a category, the models built are referred to as classification models. 

task of prediction of classes of new instances is termed as classification. 
algorithms namely Logistic Regression, Decision Trees, k- Nearest Neighbors (kNN) and Support Vector Machines (SVM), Neural Networks.

historical data to a general model, known as the classifier. 
This classifier can be arrived at by analyzing patterns in the historical data, so as to classify new instances.

--------------------------------------
Difference b/w Linear Regression and Classification is-
  In Linear regression, the target variable is continuos Example time,
  In Classification, the output of the model is Categorical Example, True, False [0,1], [X,y,z...]

-------------------------------------

Confustion Matrix - to descrive the performance of a classification model also called as a classifier
                    on a set of data for which true values are known
  
                       Predicted
                Bad                   Good
              
      Bad   True positive [TP]          False Positive [FP] - Type#1 Error
Actual
      Good   False Negative [FN]         True Negative [TN] 
              [Type-2 Error]      
Accuracy = ( True Positive + True Negative ) / Total Predictions

More advanced version of Confusion matrix is "Classification Report"
  It includes,
  a) Precision - (Number of prediction correctly predicted) / (Total number of predictions as Class A) i.e. 
     TP / (TP + FP)
  
  b) Recal - The recall for a class A indicates how good the model is in fetching/retrieving instances of class A
          i.e. (Number of instances correctly predicted as class A) / (Number of instances actually class A) 
          = TP / (TP + FN)
          
  c) F-Score - 
  To evaluate a model with a single metric based on precision and recall, 
  we can use the F-Score. This metric is the harmonic mean of precision and recall
  and can indicate how good the model is in classifying instances of a particular class. It is defined as follows
  
   = 2 * (precision(A) * recall (A)) / precision(A) + recall(A)
  
  d) Support -

   
------------------------------------

Decision Tree (Classification Algorithm) -
  A decision tree is a tree-like structure in which:
    the root node and each internal node represent a "test" on an attribute of an instance in the dataset
    the outcome of each test is represented by the corresponding branches
    the node that does not branch further is called a leaf node and represents the class labels*

A set of rules have been given which are executed on input to identify the result
The historical data has been split based on the highest percentage that reflects/impacts the decision
At the leaf node, the result should be distinct for node and its subnode
i.e.
A subset which contains instances belonging to only one class label is called a pure (homogeneous) subset. 
The predictor attribute on which the dataset is split to obtain the maximum number of pure subsets is called the best attribute

Note: Above is recusrrive, all but identified attributes will be revalued for each subtree
This is known as Gini Index calculation

----------------------------------------

kNN - K- Nearest Neighbors
 k-Nearest Neighbors (kNN) is a non-parametric learning algorithm used for classification.
to determine the class label of a new data point (instance/tuple) by 
  comparing it with existing data points (instances/tuples), that are closest to it.

How it works?
  kNN algorithm identifies the nearest neighbors based on Euclidean distance* - a commonly used distance metric. 

Note: Euclidean distance is used as a distance metric when the data tuple comprises of numeric attributes. 
     Distance metrics such as Hamming distance can be used when the data tuple comprises of categorical attributes.
     
Important - all the numeric attributes of the tuples can be normalized before they are used for computing the Euclidean distance. 
  Example - One attribute ranges from 0-50 and other predictor/attribute ranges from 10000-50000
    Where Eucledian distance will be highly influenced bigger ranges.

After Normalization, the values of both the attributes may range from 0 - 1   

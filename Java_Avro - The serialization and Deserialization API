The Apache Avro is a part of Hadoop project.

The Marshling and Unmarshling of data done using I/O stream 
 of Hadoop Implementation of WritableComparable interface like IntWritable.class
 is not standard way, in addition to that have strong dependency
 
 
==========
Avro is the solution to the challenge where we have advantage of Marshling and Unmarshling
  - A schema based utility
  

{
   "type" : "record",
   "namespace" : "com.test.avro.schemas",
   "name" : "Employee",
   "fields" : [
      { "name" : "Name", 
        "type" : "string" 
      },
      { "name" : "Age",
        "type" : "int" 
      }
   ]
}


Schema Metadata information-
  1. type -> Describes the type of content or Value
  2. namespace -> defines the name in which object resides
  3. name -> Describes the schema or subschema name
  4. fields -> attribute which contains the following
             - name -> describes the name of field
             - type -> the type of the field



Primitive datatypes in Avro -
  int
  boolean
  long
  double
  string
  null
  float
  bytes

Complex datatypes of Avro?
  record -> has name, namespace, type, fields
  enum -> name, namespace, symbols (the array of names)
  
  Example enum:
  {
   "type" : "enum",
   "name" : "OneToThree", 
   "namespace": "data", 
   "symbols" : [ "ONE", "TWO", "THREE" ]
  }
  
  array -> type, items
  
  Example 
  
  { 
    "type" : "array", 
    "items" : "int"
  }
  
  unions - When the field has One or more types 
  
  Example -
  
{
   "type" : "record",
   "namespace" : "com.test.avro.schemas",
   "name" : "Employee",
   "fields" : [
      { "name" : "Name", 
        "type" : "string" 
      },
      { "name" : "Age",
        "type" : "int" 
      }
   ]
}
  
   
  fixed
  
  { 
    "type" : "fixed" , 
    "name" : "bdata", 
    "size" : 1048576
  }
  
  map

  {
    "type" : "map", 
    "values" : "int"
  }



===============

Avro datatypes have been serialzed and deserialized :

  1. SpecificDatumWriter class:
    The class implementation converts Java Object to an in-memory serialized format
    
    method - getSpecificData() provides the SpecificData implementation used by this writer
    
    1.a DataFileWriter(DatumWriter<T>  datumWriter)
      This class writes a seq. serialized records of data to a schema, along with schema ina file
    
      append(T datum)
      appendTo(File file) -> to append to existing file
    
    
  2. SpecificDatumReader class:
    It reads the schema and determins the in-memory data representation
    
    SpecificDatumReader(Schema schema) -> contructs where writers and readers are the saame
    
    SpecificData getSpecificData()
    setSchema(Schema actual) -> To set the writer's schema
    
    
    2.a DataFileReader (File file, DatumReader<T>)
      provides a random access to file written with DataFileWriter
    
   - next() reads the next datum in the file
   hasNext() - returns true if more entries are available in file





  3. Schema.Parser -> contains the methods to parse the schema written in a file
  
    parse(File file) -> Parses the schema in the given file
    parse(InputStream in) -> Parses the schema provided in the InputStream
    parse(String s) -> Parses the schema in the given string


  4. Fetch a record -?
  
  GenericData.Record
   Object get(String key) ->  return the value of a field of given name
   Schema getSchema() -> return the schema of given instance
   void put(String key, Object value) -> set the value for the key





=============================

How to generate the Avro class based on the schema
  .avsc ---------> Utility ----------> Bean/.java file

1. Define an Avro schema

Example schema-

{
   "namespace": "com.test.avro.schema",
   "type": "record",
   "name": "EmployeeRecord",
   "fields": [
      {"name": "name", "type": "string"},
      {"name": "id", "type": "int"},
      {"name": "salary", "type": "int"},
      {"name": "age", "type": "int"},
      {"name": "address", "type": "string"}
   ]
}

2. Compile the schema using the Avro utility. A code will be generated corresponding to schema

java -jar <path/to/avro-tools-1.7.7.jar> compile schema <schema-file> <destination-folder>

Output>
A class file the contents in it-
EmployeeRecord.java

a. Default constructor, and parameterized constructor which accept all the variables of the schema.
b. The setter and getter methods for all variables in the schema.
c. getSchema() method which returns the schema.
d. Builder methods.


3. Populate the schema with the data
EmployeeRecord firstEmployee = new EmployeeRecord();
firstEmployee.setXXX(...);

4. Serilaize the data using Avro library

      //Instantiate DatumWriter class
      DatumWriter<EmployeeRecord> employeeDatumWriter = new SpecificDatumWriter<>(EmployeeRecord.class);
      DataFileWriter<EmployeeRecord> loyee = new DataFileWriter<>(employeeDatumWriter);
	
      empFileWriter.create(firstEmployee.getSchema(), new File("serialized_employee.avro"));
	
      empFileWriter.append(firstEmployee);
      empFileWriter.close();



5. Deserialize Avro Object-
DatumReader<EmployeeRecord> employeeDatumReader = new SpecificDatumReader<>(EmployeeRecord.class);
DataFileReader<emp> dataFileReader = new DataFileReader(new File("serialized_employee.avro"), employeeDatumReader);

EmployeeRecord employeeRecord = null;
while(dataFileReader.hasNext()){

   employeeRecord = dataFileReader.next(employeeRecord);
   
}


==================================================


import java.io.File;
import java.io.IOException;

import org.apache.avro.Schema;
import org.apache.avro.file.DataFileWriter;

import org.apache.avro.generic.GenericData;
import org.apache.avro.generic.GenericDatumWriter;
import org.apache.avro.generic.GenericRecord;

import org.apache.avro.io.DatumWriter;

class AVRODynamicSchemaLoadingExample {

   public void serialize() throws IOException{
	
      //Instantiating the Schema.Parser class.
      Schema schema = new Schema.Parser().parse(new File("employee.avsc"));
		
      //Instantiating the GenericRecord class.
      GenericRecord generalRecord = new GenericData.Record(schema);
		
      //Insert data according to schema
      generalRecord.put("name", "ramu");
      generalRecord.put("id", 001);
      generalRecord.put("salary",30000);
      generalRecord.put("age", 25);
      generalRecord.put("address", "chenni");
		
      GenericRecord generalRecord2 = new GenericData.Record(schema);
		
      generalRecord2.put("name", "rahman");
      generalRecord2.put("id", 002);
      generalRecord2.put("salary", 35000);
      generalRecord2.put("age", 30);
      generalRecord2.put("address", "Delhi");
		
      DatumWriter<GenericRecord> datumWriter = new GenericDatumWriter<GenericRecord>(schema);
		
      DataFileWriter<GenericRecord> dataFileWriter = new DataFileWriter<GenericRecord>(datumWriter);
      dataFileWriter.create(schema, new File("serialized_employee.avro"));
		
      dataFileWriter.append(generalRecord);
      dataFileWriter.append(generalRecord2);
      dataFileWriter.close();
		
      System.out.println("data successfully serialized");
   }
   
   // DatumReader
   // DatumFileReader
   
   public void deserialize() {
         //Instantiating the Schema.Parser class.
      Schema schema = new Schema.Parser().parse(new File("schema.avsc"));
      DatumReader<GenericRecord> datumReader = new GenericDatumReader<GenericRecord>(schema);
      DataFileReader<GenericRecord> dataFileReader = 
          new DataFileReader<GenericRecord>(new File("serialized_employee.avro"), datumReader);
     
     GenericRecord employee = null;
		
      while (dataFileReader.hasNext()) {
         emp = dataFileReader.next(employee);
         System.out.println(emp);
      }
   }
}



=========================================

Example and Mapping for the types -

*.avsc file

{
 "name": "Employee",
 "type": "record",
 "namespace": "com.example.avro.entity",
 "fields": [
  {
   "name": "avroArray",
   "type": {
    "type": "array",
    "items": "string"
   }
  },
  {
   "name": "avroMap",
   "type": {
    "type": "map",
    "values": "string",
    "doc": "A union is used to indicate that a field may have more than one type. They are represented as JSON arrays."
   }
  },
  {
   "type": [
    "int",
    "null"
   ],
   "doc": "It will create Integer not int",
   "name": "id"
  },
  {
   "type": [
    "long",
    "null"
   ],
   "doc": "It will create Long not long",
   "name": "ld"
  },
  {
   "type": "int",
   "doc": "It will create int",
   "name": "intField"
  },
  {
   "type": "long",
   "doc": "It will create long",
   "name": "longField"
  },
  {
   "doc": "aggregation",
   "name": "address",
   "type": {
    "type": "record",
    "name": "Address",
    "fields": [
     {
      "name": "street",
      "type": [
       "string",
       "null"
      ]
     },
     {
      "name": "flatNo",
      "type": "int"
     },
     {
      "name": "pincode",
      "type": "long"
     }
    ]
   }
  }
 ]
}

====================
Avro to java classes conversion

Employee.java 

   private java.util.List<java.lang.CharSequence> avroArray;
   private java.util.Map<java.lang.CharSequence,java.lang.CharSequence> avroMap;
  /** It will create Integer not int */
   private java.lang.Integer id;
  /** It will create Long not long */
   private java.lang.Long ld;
  /** It will create int */
   private int intField;
  /** It will create long */
   private long longField;
  /** aggregation */
   private com.example.avro.entity.Address address;


Address.java

   private java.lang.CharSequence street;
   private int flatNo;
   private long pincode;
   
   
 pom.xml
<build>
<plugins>
    <plugin>
     <groupId>org.apache.avro</groupId>
     <artifactId>avro-maven-plugin</artifactId>
     <version>1.8.2</version>
     <executions>
      <execution>
       <phase>generate-sources</phase>
       <goals>
        <goal>schema</goal>
       </goals>
       <configuration>
        <sourceDirectory>${project.basedir}/input</sourceDirectory>
        <outputDirectory>${project.basedir}/src/main/java/</outputDirectory>
	<!-- Note the PRIVATE specifies all the fields as private else it will be @deprecated public-->
        <fieldVisibility>PRIVATE</fieldVisibility>
       </configuration>
      </execution>
     </executions>
    </plugin>
    <plugin>
     <groupId>org.apache.maven.plugins</groupId>
     <artifactId>maven-compiler-plugin</artifactId>
     <configuration>
      <source>${java.version}</source>
      <target>${java.version}</target>
     </configuration>
    </plugin>
   </plugins>
</build>



Kafka connect is a data streaming platform
  This platforms interfaces Kafka with other systems
  - OSS under Apache Kafka distribution
  - Simple, scalable and reliable


Input channels --------->Connect ->>>>Kafka clusture ->>>>Connect ----------->Output channels

Example-
  Stream an entire SQL data from the database into Kafka
  Stream the entire topics into HDFS for analytics and processing
  Stream the data into Elastic Clusture for secondary indexing
  (CQRS -> Command query responsibility seggragation pattern for External search and queries)


Kafka Connect is using producer and Consumer API's and that automatic
  - COTS and deeply tested connect components are available for common data sources
  - Automatic loadbalancing for distributed systems
  - No "do it yourself" i.e. no external coding required
  - Easy use existing connectors and extensible for new features

The basic responsibility of Apache Connect to 
 - Trasfer data b/w Kafka and Other systems
 - Connectors recieve data from different channels into Kafka (ingress/source)
 - Connectors sends data to external systems (egress/sink)


Types of Connect Connector:
  - aleardy developed connectors can be used [COtS]
  - New connectors can be created 
  
1. File stream 
    - Read from the last of file
    - Appends Kafka messages at end of file
    
2. JDBC
  - Polls database a new or Updated rows. Creates a Kafka message for each record modified
  - Each table has separate Topic where records are published
  - New and deleted records in table have been handed automatically

Properties (topic name
  connection.url - JDBC URL
  topic.prefix - prefix to table name for generating Kafka_Topics
  mode - mode for detecting the table changes, incrementing, timestamp, timstamp+incrementing,  bulk
  query - The custom query to run
  poll.interval.ms - the frequency poll for new data
  table.blacklist - a list of tables to ignore
  table.whitelist - a list of table to specify (one of table.whitelist or blacklist should be present)

3. HDFS
  Polls Kafka and write to HDFS
  Integration with Hive (Auto table creation and Schema evalutation with Apache Avro)
  Uses Kerberos to connect with HDFS and Hive metastore
  Plug-ins for partitioner (Kafka, fields, time, custom partitioner)

4. Elastic search
5. AWS S3
...

=================================
How do you detect new or updated rows?
  - Incrementing Column: It check for the sequence number but cannot be used for updates
  - Timestamp column - Checks a single "last_modified" column but doe not guratee all updates are read
  - Timestamp and incrementing column - Combination does achieve benefit of above
  - Custom query - 

=====================

Converters create a custom format for data to serialize the stream to Kafka
Converters are decoupled from connectors to enable reuse
  - Allow any connector to work with any serialization format
  
  DataSource ->> Custom format ---> Connectors ---> Converter --[bytes]-->Kafka clusture
                                                        ||
                                                     ========
                                                     Schema registry

Converter Data formats-
  - Key converter
  - Value Converter
  
Examples -
  AvroConverter (Best practice)
  JsonConverter
  StringConverter

----------------------
It is good to use AvroConverter with SchemaRegistry with the connectors

key.converter=io.confluent.connect.avro.AvroConverter
key.converter.schema.registry.url=<URI>

value.converter=io.confluent.connect.avro.AvroConverter
value.converter.schema.registry.url=<URI>

Benefits of Using AvroConverter:
  - A datastructure format support
  - Generation of Code files from Datastructure
  - Avro stores data efficiently
  - Typechecking of data or validation
  - Avro schema updates and evolution
  
===============================
Benefits of KafkaConnect:
  . The track record of producer and consumer connctors, for fault tolerant
 
 The method of tracking offset depends on type of Connector being used
=========================
KafkaConnect modes-
  1. Standalone mode -
    Single worker process on a single machine
    Example-
    
    connect-standalone connect-standalone.properties connector1.properties [...]
    
    The argument for properties is varards specifies properties for each Connector to be started locally
    Each connector will run in seperate worker thread locally
    
  2. Distributed mode - Multiple workers distributed across machines
  
  In this mode Kafka Connect starts on each worker node
  connect-distributed connect-distributed.properties
  
 ------------
 The Group coordination b/w distributed nodes-
   . The communication is done using group membership protocol
    (Configure working with same "group.id")
 
  . Workers distribute load within this Kafka connect "cluster"
  
  
================================  
Configuring Connectors-
  . Connectors configuration depends on the mode (standalone or distributed)
  . in distributed mode, configuration can be done only via REST API
    . Changes will be applied on restart of each worker
    . The configuration is stored in a special Kafka topic\
    . group.id - to merge connectors to make a group
    . num.partitions (default: 1)
    . replication.factor (Default: 1) - best would be 3 at least
    
    /etc/kafka/connect-distributed.properties
   
  . in standalone mode, the configuration is done via properties file
    . Configuration done using REST API are temporary change
    . /erc/kafka/connect-standalone.properties
  
  . Connectors can be added, removed or modified using REST API
    . REST request can be made to any worker


GET /connectors - Get a list of active connectors
POST /connectors - Get a new connector
GET /connectors/(string: name)/config - Get configuration information for a connector
PUT /connectors/(string: name)/config - Create a new connector or update the existing config


===================================
SMT (Single message transforms) - a light weight operations performed on message
  The ability to transform a message at a time
  - Configured at the connector-level
  - Applied at message key or value level
  
Few transformations Example-
 InsertField: insert a field using attributes from the message metadata or from configured static value
 ReplaceField: rename fields, or apply filters(whitelist, blacklist)
 ValueToKey - Replace the Key with a new keyformed from a subset of fields in value payload
 TimestampRouter - Update the Topic field as a function of Original Topic value and timestamp

=================================

Administration of Kafka - 
  * Different types of installers - tar, archive, zip, RPM or docker images
  * Brokers are built as backward compatible
  * In clusture rolling upgrade is possible
  
  
-- Topics have been automatically created when first used
  Deafult (Single replication and Single Partition)

in production the auto.create.topics.enable=false. the topics should not be generated automatically

Instead of autogeneration, create topics manually-

kafka-topics --zookeeper HOST:PORT --create --topics TOPIC_NAME \
  --partition 6 --replication-factor 3
  
kafka-topics --zookeeper HOST:PORT --alter --topic TOPIC_NAME \
  --partitions 30
  
 kafka-topics --zookeeper HOST:PORT --delete --topic TOPIC_NAME
 // Stop all producers/Consumers before deleting a topic
 // All Brokers must be running for a delete to be successful


=====================================
Kafka Log management-
  [Default duration]Retain - Messages will be retained for seven days
  Retention policy can be configured using-
    log.retention.ms
    log.retention.minutes
    log.retention.hours
    log.retention.bytes - size of log
  

Kafka can store the message for ephermal time period
 - When cleaning upa a log, the default policy is delete. Removes all log segments
   where all messages are older than the retention.ms or until the log size is below 
   retention.bytes (or both)


---------------------------
Regular Topic - every event is a discrete event in time, and all the events will be tracked

Compacted Topic- A compacted log retains at least the last known message value
  for each key within partition
  Example-
   Storing state of external applications
   A DB/table change log
   in IM&P last activity done in group, somebody joined the group
   
   When does a broker call Compaction algorithm-
    log.cleaner.min.cleanable.ratio
    : ratio of dirty(uncompacted) to total log size 
    - trigger log clean if the ratio of dirty/total is larger than this value

  default (0.5)

At broker level -
  min.cleanable.dirty.ratio - A topic's property override, defined a property at Broker leveel

-------------------------

**Determine the partition sizing ?
  Default - A single partition and a single broker replica
  If a Topic is manually created, you can specify the number of Partitions,
   and the number of Replicas per partition
   
 
 kafka-topics --zookeeper HOST:PORT --create --topic TOPIC_NAME \
  --partitions 6 --replication-factor 3

Note: The number of partitions has impact on performance

Parallelization -
  Produces can write to multiple partitions in parallel
  - More partitions can result in better throughput when sending data to cluster
  
  The performance Challenges?
    1) If Broker fails  and it is leader for some partitions, each of these partitions 
    will be unavailable until leader is elected
    
    * The more partitions for a leader, the longer leader election will take
  
    2) More partitions means increase in End-to-End latency [time from producing a message to it being read by Consumer]
     - A broker uses a single thread to thread for data replication b/w shared partitions
       across multiple brokers
       
     - Larger clustures where replicas are distributed amongst more Brokers
     
     3) Memory requirements(buffers on clients per partition)
      - Each parition need a separate bugger on client i.e. increases with number of partitions
            mitigation max.poll.records 

=======================================

Kafka Security?
  Encyption, Authentication (Who you are?) and Authorization (What you are allowed to do?)

Note:kafka brokers can listen on multiple ports

  1) Plain text (no wire encryption or authentication)
  2) SSL  (wire encryption and Optional SSL authentication)
  3) SASL - Simple Authentication and Security Layer
    . Connects to external Kerberos server
    - Salted and Hashed passwords
    - clear text user/passwords
   
   SSL has huge impact on performance of Broker and Clients
   
  
Authorization?
  Kafka supports ACLs
  Example-
    Allow Producers to write to specific Topics
    - Consumsers to read from Specific Topics
    Allow certain users to Administer
    
 How to modify ACLs
  kafka-acls --authorizer-properties zookeeper.connect=HOSTNAME:PORT \
  --add --allow-principal User:Jatish --operation Read --operation write \
  --topic TOPIC_NAME
  
-----------------------------------------
The Schema registry can be confnigured with securitu\y

- Secure communication and authentication b/w Schema registry and the Kafka cluster
 * SSL (Secure communication for end-user REST API calls [HTTPS])
 * SASL (Authentication with Zookeeper)

Note: Schema evolution or updated is an administrative action. All other users have read-only access






































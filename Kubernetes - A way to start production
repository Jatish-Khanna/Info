
What containers brings to Us?
  1) Protability
  2) High boot up time 
  3) Lower resource requirements when compared to VM
  4) Easy deployment 
  5) Software virtualization over h/w virtualization
  6) Isolation 
  7) Easy scalability and orchestration
  
  Upgrades with Zero downtime, Sequentionally, pause, resume, rollback.
  
  Container Orchestration Engine - K8s?
    To automate deploy, scale and manage containerized applications on group of servers
    
      Fault tolerant 
      Availabililty 
      Scalability - in, out, up and down
      Clustering
      Scheduling
      Loadbalancing
      Deployment and rollout or canary

Advanced:
    Service Discovery
    Monitoring 
    Secrets managements

Borg - Propprietry container manager of Google

============================================
Creating A K8s cluster:
  Max. 5000 Nodes per cluster
  - Worker nodes[Minions] are the VMs or physical machines where the actual images are deployed
  - Master - Contoller that supports features for user and worker nodes
  - Pods - In most cases we see one containers per pod but it can have more containers
  - Containers - runtime environments that serve world
  
Master:
  1) API server - gateway of entire cluster, to view and review the metadata though APIs.
    How, kubeclt or UI or other ways to invoke APIs
  
  2) Scheduler - scheduling pods for workers
  
  3) Control manager - 
    Nodes are running, orchestration
  
    a) Node controller
    b) Duplication controller
    c) Service account and Endpoint controller
    d) End point controller
  
  4) etcd - Key/Value database to store the metadata and runtime state of cluster


Worker nodes:
 An VMs or physical server that runs the container on runtime environment E.g. Docker or Rocket[RKT]
 a) Kubelet Agent - node agent to monitor PODS and makesure they are healthy. To restart the nodes
 b) Kube proxy - Distrubted n/w manager across all the nodes, pods, containers. 
 c) Runtime container 
 d) Pod - Each POD contain one or more container.

=============================================

Deployments::
  a) Replicas (Scale in or Out) - defaults:1 - atleast one Pod is running at single point in time.
  b) Upgrade - 
  c) Rollback - 
  d) Scale Up or Down - 
  e) Pause and resume - 

Types:
  1. Recreate - Downtime is allowed, when switcting from V1 to V2
  2. Rolling  update[Default] - Incremental, change to all the Pods
  3. Canary - Gradual shift from V1 to V2
  4. Blue/Green - Both the version are deployed and a traffic switch is done at the LB level.
  
  Sample manifest file:
  
   apiVersion: api/v1
   kind: Deployment
   metadata:
     name: redis
     label:
       app: redis
   spec:
     replicas: 3
     selector:
       matchLabels:
         app: redis  
     
     template:
       metadata:
         labels:
           app: redis

      spec:
        containers:
          - name: redis
            image: redis:latest
            ports:
              - containerPort: 6379

===========================

kubectl create -f redis-deploy.yml
kubectl get deploy // get deplyment
kubectl get rs  // Get replica ser
kubectl get po  // Get pods
kubectl describe deploy redis

1) Using kubectl
kubectl set image deploy redis redis-container=redis:5.0.6 --record // Upgrade with --record to track history
2) Edit deployment file
kubectl edit deploy redis

kubectl rollout status deployment/redis
kubectl get deploy // get deplyment

kubectl rollout history deployment/redis

Rollback:
kubectl rollout undo deployment/redis
kubectl rollout status deployment/redis


Scale up or Down:
  kubectl scale deployment redis --replicas=5
  
  kubectl scale deployment redis --replicas=3

Delete deployment:
kubectl delete -f redis.yml

==========================================================

Handson:

Minikube -> A utility to test Kubernetes as standalone system. Which includes K8s master and Worker nodes
Kubeadm -> A utility to setup K8s cluster in realtime. 

  E.g. 
  
  1. Initialize the cluster master node
   kubeadm init --apiserver-advertise-address $(hostname -i)

kubectl -> Manage the K8s cluster from the command line. Pods, deployments and services
  
Commands:
 create , get
 describe, delete,
 exec, logs
 edit, run
 apply, scale
 
 Types:
 Type -    abbrevation
   pod(s) - po
   deployment(s) - deploy
   replicaset(s) - rs
   replicacontroller(s) - rc
   deamonset(s) - ds
   namespace(s) - ns
   persistvolume(s) - pv
   persistvolumentclaim(s) - pvc
   job(s) - --
   cronjob(s) - --
 
 kubectl [command] [TYPE] [NAME] [flags]

Examples:
  kubectl CREATE -f my-stack.yml[json] # the manifest file can be in JSON or YML format
  
  kubectl CREATE -f <directory> # to read all the files from the directory

=====================
Display resource-

  kubectl get po/pods/pod <name_of_pod>
  kubectl get po/pods/pod <name_of_pod> -o wide # to get wide output with the worker node details
  kubectl get po,deploy         # to display multiple resource types

=========================
Kubectl describe - complete description, state, events of resource 

  kubectl describe po <pod_name>
  kubectl describe nodes <node_name>

========================
kubectl delete - to delete the resources

  kubectl delete -f pod.yml # delete resource created by manifest file
  kubectl delete pods,services -l name=<label_name>      # delete pods and services with label
  kubectl delete pods --all            # to delete all the resources of type=pod
=========================
kubectl exec - to interact with the container

  kubectl exec <pod_name> date  # to exec date utility with pod
  kubectl exec <pod_name> -c <container_name> date    # to interact with specific container in a single pod

  kubectl exec -it <pod_name> /bin/bash  
===============================

kubectl logs --- print logs of container in a pod

  kubectl logs <pod_name>
  kubectl logs <pod_name> -f

=================================

----------------------- Kubenetes Cluster -------------------

 You can bootstrap a cluster as follows:

 1. Initializes cluster master node:

 kubeadm init --apiserver-advertise-address $(hostname -i)


 2. Initialize cluster networking:

 kubectl apply -n kube-system -f \
    "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 |tr -d '\n')"

It will install weave network plugin for networking b/w components



  3. Join the worker nodes to the cluster -
  
 You can now join any number of machines by running the following on each node
as root:

  kubeadm join 192.168.0.23:6443 --token agzaoz.173lih2kuhqrpr6x 
  --discovery-token-ca-cert-hash sha256:6db4857e68df5ae8cde614f6b4b5c78e3b02e301993eb25c231a7f3419dd8256

 4. First deployment
 
  kubectl run kubebootcamp --image=gcr.io/google-samples/kubernetes-bootcamp:v1  --port=8080

==========================================

kubeadm init [flag]   // to initialize master
kubeadm join --token [] --discovery-token-ca-cert-hash [] // to initialize the worker node
 kubeadm token [CREATE|DELETE|LIST|GENERATE]
 kubeadm version
 kubeadm upgrade plan [version] [flags]  // upgrade the Kubernetes cluster

=================================

Kubernetes cluster imporve performance - 
  
  swapoff -a
  setenforce 0 OR sed -i 's/enforcing/disabled/g' /etc/selinux/config
  
  ## reboot all nodes
  systemctl start docker 
  systemctl enable docker
  systemctl status docker
  
kubeadm - 
kubelet -
kubectl - 

kubeadm token CREATE --print-join-command  # to print the join command for the worker node.

==================================

Lifecycle of a POD:
  Manifest at Masternode -> Pending [] -> Creating Pod -> Running -> succeeded/Failed 

Note: The pods are replaced with new Pods

======================

Pod Config or Manifest file?


apiVersion: v1
kind: Pod
metadata:
  name: <name_of_object-here-pod>
  labels:
    app: test
    mode: dev

spec:
  containers:
    - name: <name_of_object-here-pod>
      image: redis:latest




====================

apiVersion: #define the version number which Kubernetes version Object belogs to
  Kind is mapped with Version of manifest file

  v1: Pod, ReplicationController, Service
  apps/v1: ReplicaSet, Deployment, DaemonSet
  batch/v1:  Job


metadata:
  labels: help to filter out the pods when executing command
  
  
spec:
  to define the container configuration for a pod


Create and Deploy the pod using manifest
  kubectl create -f redis-manifest.yml
  kubectl get pod -l dev  # all the pods with label dev
  kubectl get pod -o wide|yaml|json
  
Details of the pod
 
  kubectl describe <pod_name>
  
==================================
ReplicaSet -
  To balance the PODs to the required number of running instance at single point in time. Min=1
  balance= Create new or Delete additional, scale in or out  
  
  The labels are used to manage the pods by replication controller.
 
apiVersion: v1
kind: ReplicationController
metadata: 
  name: redis-replca-controller 
spec:
  replicas: 3
  selector:
    app: <labels_in_template_section>
  
  template:
    ... # all the section same as the first manifest file
    name: 
    labels:
      x: y
 spec:
   containers:
     - name :
       image: 
       ports:
       - containerPort: 6376
 
 # labels and selectors are used to map Pods to replica set

=========================

Scale in and Out app instance:
  kubectl scale rc <name_of the_rc_controller> --replicas=5

==========================

Replica Set: specifies the required number of pods running at samepoint in time.
 and is a next generation of Replication Controller [+ Set based Selectors - Equiality based selectors]

What are Selectors?
  Controllers and services manage pods using selectors. These are the pod labels
  
  Define Selectors:
  a) Equality based selectors
    Operators:
    =        - same as ==
    ==       - same as -
    !=       -
    
    Example:
      environment != production
      tier == backend
  
  kubectl get po -l environment != production
  
  OR in manifest file
  
  selector:
    environment: production
  
  b) Set based selectors
     in
     notin
     exists
  
    Example:
      environment in (dev, qa)
      tier notin (frontend)
      
    kubectl get po -l 'environment notin (dev, qa)'
  
  OR in manifest file

  selector:
    matchExpressions:
      - {key: environment, operator: notin, values: [dev, qa]}


Note:
  selector:
    key: value 
    The above sample is being used for older resources such as ReplicationController, Services
    
  selector:
    matchLabels:
      key: value
   The above sample is being used for newer resources such as ReplicaSet, Deployments, Job, Deamon Set

=======================================

Example using ReplicaSet:
 
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: redis
spec:
  replicas: 3 
  selector:
    matchLabels:
      environment: prod
    matchExpressions:
      - {key: tier, operator: Notin, values:[frontend]}
  template:
    metadata:
      name: 
      labels:
    spec:
      containers:
        - name:
          image:
          ports:
            - containerPort:

kubectl create -f redis-rs.yaml
kubectl get po
kubectl get po -l 'environment notin (dev,qa)'
kubectl get po -o wide
kubectl describe rs redis
================================

Upgrades with Kubernetes---

  Deployment: A controller like other controllers in the Kubernetes e.g. ReplicaSet
    Default replicas with each deployment if not specified is One
    Helps with different Upgrade strategies - 
    Rollback strategy 
    Scale In and Out
    Pause and Resume the deployment process
  
Upgrades-
  a) Recreate - Shutdown and Create fresh instance
  b) Rolling update - Incremental upgrade, Add one and then remove (default)
  c) Canary - Gradual shift of instances [ideal method]
  d) Blue/Green - V1 and V2 are running - loadbalancer switched from V1 to V2

Example manifest:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-deploy
  labels:
    app: redis
    tier: backend
spec:
  replicas: 3
  selectors:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
       ....

## Note labels are used to filter resources but name

kubectl CREATE -f redis-deploy.yaml
kubectl get po
kubectl get po -l app=redis
kubectl get rs -l app=redis
kubectl describe deploy redis-deploy
===============================================

Sample Upgrade example:
1) The container version is upgraded
  kubectl set image deploy redis-deploy redis-container=redis:5.0.9 --record

2) Configuration is manifest
  kubectl edit deploy redis-deploy
  
  Make changes in the deployment, save and close it. As soon as it has been closed a new deployment will be rolled out  
  kubectl rollout status deployment/redis-deploy
  kubectl rollout history deployment/redis-deploy
===========================  
  Rollback the Upgrade on failure-
  kubectl rollout undo deployment/redis-deploy
  
=====================
Scale in and out instance:
  kubectl scale deployment redis-deploy --replicas=5
  kubectl get deploy
=========================

Services in Kubernetes:
  possible to have to static IP for the pods?
  How do different pods communicate with each other?
  How to expose application to real world?
  
  Services? Way of grouping of Pods running on the cluster. 
    Load balancing,
    Service discovery b/w apps
    Zerodowntime app deployments
    
    User |||  <-----Frontend Service------>        Front End Pods <--------BackEnd Service ---->      |||| Back end Pods
  
  Labels and Selectors help to identify the pods to create connection b/w them.
  
  Type of Service:
  1) Cluster Ip - 
    Service reachable only within the cluster.
    E.g. Connection Backend to Frontend pods.
    
  2) NodePort -
    Expose webapp to outside world
  
  3) Load balancer 
    To identify the Pod to serve the request.

=========================================

Expose the Deployment to Outside world - Node Port Service
  A node has a pod running with IP address but 
  a) POD IP is dynamic or Ephemeral IP
  b) No connectivity b/w External world and the Pod
  
  User (access - Nodeip:node port) ----(Node IP: Node Port)----> NodePort Service --(Pod IP: Pod Port)--------> Pod ---(App port)----> Application

**Limitation: Manual defination of port in NodePort service only allows port range from [30000 to 32767]
  Or Let Kubernetes allocate the port dynamically
  One service per port
  Node or VM IP changes it has to be explicitly managed

Note: This service is not directly used for Production grade applications

Example manifest file:
apiVersion: v1
kind: Service
metadata:
  name:
  labels:
spec:
  replicas:
  selectors:
    app: redis
    environment: production
  type: NodePort
  ports:
    - nodePort: 30000
      port: 80
      targetPort: 80
  template:

nodePort is Optional, port defines the Service to Pod port mapping and targetPort is the actual port where application is listening
  targetPort is same as containerPort
  
kubectl CREATE -f redis-deploy.yaml
kubectl CREATE -f redis-svc-np.yaml  # NodePort service to be created
kubectl get po -o wide  # Gives you POD Ip : Pod Port
kubectl get svc # Gives you NodePort Service IP and NodePort Service port
kubectl get po -o wide  # Gives you External IP of Node
kubectl descrive svc redis-svc-np  # Give you Service Port exposed to User

====================================  

Is NodePort Service Sufficient?
  - Node Port and NodeIp service to be provided to EndUser but again the Ips are Ephemeral
  
  Solution- User the LoadBalancer Service
  Expose the App to Internet using the LoadBalancer
  
  Example manifest file:
  apiVersion: v1
  kind: Service
  metadata:
    name:
    labels:
  spec:
    replicas:
    selectors: 
      environment: prod
    type: LoadBalancer
    ports:
      - nodePort:
        port:
        targetPort:
    template:

## above manifest as a part of command
kubectl expose deploy redis-deploy --name=redis-service-lb --port=80 --target-port=80 --type=LoadBalancer

kubectl CREATE -f redis-svc-lb.yaml  # NodePort service to be created
kubectl get po -o wide  # Gives you POD Ip : Pod Port
kubectl get svc # Gives you NodePort Service IP and NodePort Service port
kubectl get po -o wide  # Gives you External IP of Node
kubectl descrive svc redis-svc-np  # Give you Loadbalancer IP

=====================================

ClusterIp Service:
  Example: To restrict the BackEnd pods to be connected from outside world but front end Pods
  This is the default service of Kubernetes.
  
Manifest file for Redis master:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
  labels:
    app: redis-master
    type: server
spec:
  replicas: 1
  selector:
  template:
    metadata:
      labels:
        app: redis-master
        tier: backend
        type: server
    spec:
      containers:
        image: redis:latest
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
          - containerPort: 6379
  --------------------------------------
  
  Manifest Redis clients:
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: redis-slave
    labels:
      app: redis-slave
      type: server
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: redis-slave
        type: server
        tier: backend
    template:
      metadata:
        labels:
          app: redis-slave
          type: server
          tier: backend
      spec:
        containers:
          - name: redis-slave
            image: gb-redisslave:v1
            resources:
              requests:
                cpu:
                memory:
            ports:
              - containerPort: 6379
        
---------------------------------------
  Manifest Frontend application:
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend-app
    labels:
      app: frontend-app
      type: application
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: frontend-app
        type: application
        tier: frontend
    template:
      metadata:
        labels:
          app: frontend-app
          type: application
          tier: frontend
      spec:
        containers:
          - name: frontend-app
            image: gcr.io/google-sample/db-frontend:v4
            resources:
              requests:
                cpu:
                memory:
            ports:
              - containerPort: 80
 
---------------------------------------
  Manifest ClusterIP Service for connection b/w master, frontend and slaves:
  
  apiVersion: v1
  kind: Service
  metadata:
    name: redis-master-svc
    labels:
      app: redis-master
      type: server
  spec:
    type: ClusterIP
    selector:
      app: redis-master
      type: server
    ports:
      - port: 6379
        targetPort: 6379
  
  ---------------------------------------
  Manifest ClusterIP Service for connection b/w frontend and slaves:
  
  apiVersion: v1
  kind: Service
  metadata:
    name: redis-slave-svc
    labels:
      app: redis-slave
      type: server
      tier: backend
  spec:
    type: ClusterIP
    selector:
      app: redis-slave
      type: server
      tier: backend
    ports:
      - port: 6379
        targetPort: 6379
 
---------------------------------------
  Manifest LoadBalancer Service for connection b/w frontend and Users:
  
  apiVersion: v1
  kind: Service
  metadata:
    name: redis-lb-svc
    labels:
      app: frontend-app
      type: application
      tier: frontend
  spec:
    type: LoadBalancer
    selector:
      app: frontend-app
      type: application
      tier: frontend
    ports:
      - port: 80
 
 
 Verify:
 
 kubectl get po -l tier=backend
 kubectl get po -l tier=frontend
 kubectl get svc -l tier=backend
 kubectl get svc -l tier=frontend
=================================

Kubernetes Volumes
  - emptyDir
  - hostPath
  - gcePersistentDisk
  - awsElasticBlockStore
  - secret
  ...

Pods are ephemeral and stateless, so volumes persist data for pods
As pods can be attached to multiple containers , all the containers can have access to there volumes
Kuberenetes Volumes are associated to Pods, the data is preserved when container is restarted in the pod
  The reason to this is Kubernetes volumes are associated to Pods but containers
  
 Volume Types:
  1) Ephemeral volumes: Until the lifecycle of the pod
  2) Durable volumes: Until the Volume or disk is available

=============================

emptyDir Volume Kubernetes:
  An empty directory is created once the volume is attached to pod
  On deleting Pod the data is erased from the volume
  Primary use is - temporary space, or sharing b/w nodes
  
hostPath Volume Kubernetes:
  - A host directory is mounted on the directory inside the pod (same as docker volumes)
  - It is durable, even after the Pod is removed
  - Caveats -  In a cluster each node will have its own hostPath volume for the specific Pod i.e. the data is not shared
  - NFS to be used instead of local directory, that to be mounted on all the nodes in a cluster
  
 gcePersistentDisk Kubernetes:
   - Mounts GCE persistent disk on the pod
   - Durable same as hostPath
   - Read-write on any nodes in the cluster
   - The nodes which are using GCE persistent disk should be GCE VMs
   - Same zone and project of PV as of GCE VMs
   
   Note: GCE persistent disk must be available before use
   
=============================================

Volume Type - emptyDir ?
  Initially Empty, and ephemeral tied to Pod lifecycle. With Pod deletion the data volume will be purged or cleaned
  
ExampleManifest File:
apiVersion: v1
kind: Pod
metadata:
  name: emptyDir-vol
spec:
  containers:
    image: gcr.io/test-redis/redis
    name: emptyDir-container
    volumeMounts:
      - name: emptyDir-vol-pod
        mountPath: /opt
    
  volumes:
    - name: emptyDir-vol-pod
      emptyDir: {}

kubectl CREATE -f test-pod.yaml
kubectl get po
kubectl exec emptyDir-vol df /opt
kubectl describe pod emptyDir-vol
kubectl delete -f test-pod.yaml

Note: it is useful for storing the temporary data
=================================

hostPath - Volumes [kubernetes]
  mounts a directory of worker node as a Volume inside the POd
  - similar to the docker volume [the data is durable even when the Pod is terminated]
  Cautions: when using the cluster, else each node will have an individual volumes
    The above can be taken care of NFS or external mount path on host
    
 Example manifest file:
apiVersion: v1
kind: pod
metadata:
  name: hostPath-vol
spec:
  conntainers:
    - name: hostPath-container
      image: gcr.io/redis/redis
      volumeMounts:
        - name: hostPath-pod-vol
          mountPath: /opt
  volumes:
    - name: hostPath-pod-vol
      hostPath:
        path: /data/app

kubectl CREATE -f test-pod.yaml
kubectl get po
kubectl exec hostPath-vol df /opt
kubectl describe pod hostPath-vol
kubectl delete -f test-pod.yaml

======================================


